{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import copy\n",
    "from copy import deepcopy\n",
    "import numpy as np\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "import torch.utils.data\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "from sklearn.metrics import roc_auc_score\n",
    "import torch.nn.functional as F"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 1. Utility Functions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_tta_transforms():\n",
    "    # Define a transformation that randomly masks some features\n",
    "    class RandomFeatureMasking:\n",
    "        def __init__(self, p=0.1):\n",
    "            self.p = p\n",
    "\n",
    "        def __call__(self, x):\n",
    "            # x is a tensor of shape (batch_size, num_features)\n",
    "            # Generate a mask with probability p of masking each feature\n",
    "            mask = (torch.rand_like(x.float()) > self.p).long()\n",
    "            x_transformed = x * mask\n",
    "            return x_transformed\n",
    "\n",
    "    return RandomFeatureMasking(p=0.1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {},
   "outputs": [],
   "source": [
    "def weight_init(layers):\n",
    "    for layer in layers:\n",
    "        if isinstance(layer, nn.BatchNorm1d):\n",
    "            layer.weight.data.fill_(1)\n",
    "            layer.bias.data.zero_()\n",
    "        elif isinstance(layer, nn.Linear):\n",
    "            n = layer.in_features\n",
    "            y = 1.0 / np.sqrt(n)\n",
    "            layer.weight.data.uniform_(-y, y)\n",
    "            layer.bias.data.fill_(0)\n",
    "            # nn.init.kaiming_normal_(layer.weight.data, nonlinearity='relu')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "CoTTA Utility Functions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "metadata": {},
   "outputs": [],
   "source": [
    "def copy_model_and_optimizer(model, optimizer):\n",
    "    \"\"\"Copy the model and optimizer states for resetting after adaptation.\"\"\"\n",
    "    model_state = deepcopy(model.state_dict())\n",
    "    model_anchor = deepcopy(model)\n",
    "    optimizer_state = deepcopy(optimizer.state_dict())\n",
    "    ema_model = deepcopy(model)\n",
    "    for param in ema_model.parameters():\n",
    "        param.detach_()\n",
    "    return model_state, optimizer_state, ema_model, model_anchor\n",
    "\n",
    "def load_model_and_optimizer(model, optimizer, model_state, optimizer_state):\n",
    "    \"\"\"Restore the model and optimizer states from copies.\"\"\"\n",
    "    model.load_state_dict(model_state, strict=True)\n",
    "    optimizer.load_state_dict(optimizer_state)\n",
    "\n",
    "def configure_model(model):\n",
    "    \"\"\"Configure model for use with CoTTA.\"\"\"\n",
    "    model.train()\n",
    "    model.requires_grad_(False)\n",
    "    for m in model.modules():\n",
    "        if isinstance(m, (nn.Embedding, nn.BatchNorm1d)):\n",
    "            m.requires_grad_(True)\n",
    "            if isinstance(m, nn.BatchNorm1d):\n",
    "                m.track_running_stats = False\n",
    "                m.running_mean = None\n",
    "                m.running_var = None\n",
    "    return model\n",
    "\n",
    "def collect_params(model):\n",
    "    \"\"\"Collect all trainable parameters.\"\"\"\n",
    "    params = []\n",
    "    names = []\n",
    "    for nm, m in model.named_modules():\n",
    "        for np_, p in m.named_parameters():\n",
    "            if p.requires_grad:\n",
    "                params.append(p)\n",
    "                key = f\"{nm}.{np_}\" if nm else np_\n",
    "                names.append(key)\n",
    "                print(f\"Parameter to adapt: {key}\")\n",
    "    return params, names"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Custom Binary Entropy Function"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "metadata": {},
   "outputs": [],
   "source": [
    "def binary_entropy(logits, logits_ema):\n",
    "    \"\"\"Entropy for binary classification.\"\"\"\n",
    "    p = torch.sigmoid(logits)\n",
    "    p_ema = torch.sigmoid(logits_ema)\n",
    "    entropy = -0.5 * (p_ema * torch.log(p + 1e-8) + (1 - p_ema) * torch.log(1 - p + 1e-8))\n",
    "    entropy -= 0.5 * (p * torch.log(p_ema + 1e-8) + (1 - p) * torch.log(1 - p_ema + 1e-8))\n",
    "    return entropy.squeeze()\n",
    "\n",
    "def update_ema_variables(ema_model, model, alpha_teacher):\n",
    "    \"\"\"Update EMA model parameters.\"\"\"\n",
    "    for ema_param, param in zip(ema_model.parameters(), model.parameters()):\n",
    "        ema_param.data.mul_(alpha_teacher).add_(param.data * (1 - alpha_teacher))\n",
    "    return ema_model"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "CoTTA Class"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "metadata": {},
   "outputs": [],
   "source": [
    "class CoTTA(nn.Module):\n",
    "    \"\"\"CoTTA adapts a model by entropy minimization during testing.\"\"\"\n",
    "    def __init__(self, model, optimizer, steps=1, episodic=False):\n",
    "        super().__init__()\n",
    "        self.model = model\n",
    "        self.optimizer = optimizer\n",
    "        self.steps = steps\n",
    "        assert steps > 0, \"CoTTA requires >= 1 step(s) to forward and update\"\n",
    "        self.episodic = episodic\n",
    "\n",
    "        self.model_state, self.optimizer_state, self.model_ema, self.model_anchor = \\\n",
    "            copy_model_and_optimizer(self.model, self.optimizer)\n",
    "        self.transform = get_tta_transforms()\n",
    "\n",
    "    def forward(self, x):\n",
    "        if self.episodic:\n",
    "            self.reset()\n",
    "        for _ in range(self.steps):\n",
    "            outputs = self.forward_and_adapt(x, self.model, self.optimizer)\n",
    "        return outputs\n",
    "\n",
    "    def reset(self):\n",
    "        if self.model_state is None or self.optimizer_state is None:\n",
    "            raise Exception(\"Cannot reset without saved model/optimizer state\")\n",
    "        load_model_and_optimizer(self.model, self.optimizer,\n",
    "                                 self.model_state, self.optimizer_state)\n",
    "        self.model_state, self.optimizer_state, self.model_ema, self.model_anchor = \\\n",
    "            copy_model_and_optimizer(self.model, self.optimizer)\n",
    "\n",
    "    @torch.enable_grad()\n",
    "    def forward_and_adapt(self, x, model, optimizer):\n",
    "        outputs = self.model(x)\n",
    "        self.model_ema.train()\n",
    "        # Teacher Prediction\n",
    "        anchor_prob = torch.sigmoid(self.model_anchor(x)).detach()\n",
    "        standard_ema = self.model_ema(x)\n",
    "        # Augmentation-averaged Prediction\n",
    "        N = 32\n",
    "        outputs_emas = []\n",
    "        to_aug = anchor_prob.mean() < 0.1  # Adjusted for binary classification\n",
    "        if to_aug:\n",
    "            for i in range(N):\n",
    "                outputs_ = self.model_ema(self.transform(x)).detach()\n",
    "                outputs_emas.append(outputs_)\n",
    "            outputs_ema = torch.stack(outputs_emas).mean(0)\n",
    "        else:\n",
    "            outputs_ema = standard_ema\n",
    "        # Student update\n",
    "        loss = binary_entropy(outputs, outputs_ema.detach()).mean()\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "        optimizer.zero_grad()\n",
    "        # Teacher update\n",
    "        self.model_ema = update_ema_variables(self.model_ema, self.model, alpha_teacher=0.999)\n",
    "        # Stochastic restore\n",
    "        for nm, m in self.model.named_modules():\n",
    "            for np_, p in m.named_parameters():\n",
    "                if p.requires_grad:\n",
    "                    mask = (torch.rand(p.shape) < 0.001).float().to(p.device)\n",
    "                    with torch.no_grad():\n",
    "                        key = f\"{nm}.{np_}\" if nm else np_\n",
    "                        p.data = self.model_state[key] * mask + p * (1. - mask)\n",
    "        return outputs_ema"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 3. Data Handling Functions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "metadata": {},
   "outputs": [],
   "source": [
    "def parse_line(line):\n",
    "    parts = line.strip().split('\\t')\n",
    "    label = int(parts[0])\n",
    "    # Skip parts[1] (the second element)\n",
    "    feature_ids = []\n",
    "    for item in parts[2:]:\n",
    "        feature_id = int(item.split(':')[0])\n",
    "        feature_ids.append(feature_id)\n",
    "    return label, feature_ids\n",
    "\n",
    "def build_feature_mapping(file_paths):\n",
    "    feature_set = set()\n",
    "    for file_path in file_paths:\n",
    "        with open(file_path, 'r') as f:\n",
    "            for line in f:\n",
    "                _, feature_ids = parse_line(line)\n",
    "                feature_set.update(feature_ids)\n",
    "    feature_list = sorted(feature_set)\n",
    "    feature_id_map = {feature_id: idx + 1 for idx, feature_id in enumerate(feature_list)}  # Start from 1\n",
    "    return feature_id_map, len(feature_id_map) + 1  # +1 to account for padding index 0\n",
    "\n",
    "def compute_max_length(file_paths):\n",
    "    max_length = 0\n",
    "    for file_path in file_paths:\n",
    "        with open(file_path, 'r') as f:\n",
    "            for line in f:\n",
    "                _, feature_ids = parse_line(line)\n",
    "                max_length = max(max_length, len(feature_ids))\n",
    "    return max_length"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 4. Custom IterableDataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "metadata": {},
   "outputs": [],
   "source": [
    "class YOYIDataset(torch.utils.data.IterableDataset):\n",
    "    def __init__(self, file_path, max_length, feature_id_map):\n",
    "        super(YOYIDataset).__init__()\n",
    "        self.file_path = file_path\n",
    "        self.max_length = max_length\n",
    "        self.feature_id_map = feature_id_map\n",
    "\n",
    "    def __iter__(self):\n",
    "        worker_info = torch.utils.data.get_worker_info()\n",
    "        if worker_info is None:\n",
    "            # Single-process data loading\n",
    "            yield from self._data_generator(self.file_path)\n",
    "        else:\n",
    "            # Multi-process data loading\n",
    "            total_workers = worker_info.num_workers\n",
    "            worker_id = worker_info.id\n",
    "            # Split workload among workers\n",
    "            yield from self._data_generator(self.file_path, worker_id, total_workers)\n",
    "\n",
    "    def _data_generator(self, file_path, worker_id=0, total_workers=1):\n",
    "        with open(file_path, 'r') as f:\n",
    "            for idx, line in enumerate(f):\n",
    "                if idx % total_workers != worker_id:\n",
    "                    continue\n",
    "                label, feature_ids = parse_line(line)\n",
    "                # Map feature IDs\n",
    "                mapped_feature_ids = [self.feature_id_map.get(fid, 0) for fid in feature_ids]\n",
    "                # Pad feature_ids to max_length\n",
    "                padded_feature_ids = mapped_feature_ids + [0] * (self.max_length - len(mapped_feature_ids))\n",
    "                x = torch.tensor(padded_feature_ids, dtype=torch.long)\n",
    "                y = torch.tensor(label, dtype=torch.float32)\n",
    "                yield x, y\n",
    "\n",
    "def create_data_loader(file_path, max_length, batch_size, num_workers, feature_id_map):\n",
    "    dataset = YOYIDataset(file_path, max_length, feature_id_map)\n",
    "    data_loader = DataLoader(dataset, batch_size=batch_size, num_workers=num_workers)\n",
    "    return data_loader"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 5. Model Definitions\n",
    "### DeepFM Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "metadata": {},
   "outputs": [],
   "source": [
    "class DeepFM(nn.Module):\n",
    "    def __init__(self, feature_nums, field_nums, latent_dims, output_dim=1):\n",
    "        super(DeepFM, self).__init__()\n",
    "        self.field_nums = field_nums\n",
    "        self.latent_dims = latent_dims\n",
    "        self.feature_nums = feature_nums\n",
    "\n",
    "        # Linear part\n",
    "        self.linear = nn.Embedding(self.feature_nums, output_dim, padding_idx=0)\n",
    "\n",
    "        # FM embedding\n",
    "        self.feature_embedding = nn.Embedding(self.feature_nums, self.latent_dims, padding_idx=0)\n",
    "        nn.init.xavier_uniform_(self.feature_embedding.weight.data)\n",
    "\n",
    "        # MLP\n",
    "        deep_input_dims = self.field_nums * self.latent_dims\n",
    "        layers = []\n",
    "\n",
    "        neuron_nums = [300, 300, 300]\n",
    "        for neuron_num in neuron_nums:\n",
    "            layers.append(nn.Linear(deep_input_dims, neuron_num))\n",
    "            # layers.append(nn.BatchNorm1d(neuron_num))  # Uncomment if needed\n",
    "            layers.append(nn.ReLU())\n",
    "            layers.append(nn.Dropout(p=0.2))\n",
    "            deep_input_dims = neuron_num\n",
    "\n",
    "        layers.append(nn.Linear(deep_input_dims, 1))\n",
    "\n",
    "        # Initialize weights\n",
    "        weight_init(layers)\n",
    "\n",
    "        self.mlp = nn.Sequential(*layers)\n",
    "\n",
    "    def forward(self, x):\n",
    "        # Linear Part\n",
    "        linear_out = torch.sum(self.linear(x), dim=1)  # Shape: (batch_size, output_dim)\n",
    "\n",
    "        # FM Part\n",
    "        embedding_x = self.feature_embedding(x)\n",
    "        square_of_sum = torch.sum(embedding_x, dim=1) ** 2\n",
    "        sum_of_square = torch.sum(embedding_x ** 2, dim=1)\n",
    "        ix = 0.5 * torch.sum(square_of_sum - sum_of_square, dim=1, keepdim=True)  # Shape: (batch_size, 1)\n",
    "\n",
    "        # Deep Part\n",
    "        deep_out = self.mlp(embedding_x.view(-1, self.field_nums * self.latent_dims))  # Shape: (batch_size, 1)\n",
    "\n",
    "        # Output\n",
    "        out = linear_out + ix + deep_out  # Shape: (batch_size, 1)\n",
    "\n",
    "        return out  # Return logits"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "FNN Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "metadata": {},
   "outputs": [],
   "source": [
    "class FNN(nn.Module):\n",
    "    def __init__(self, feature_nums, field_nums, latent_dims):\n",
    "        super(FNN, self).__init__()\n",
    "        self.field_nums = field_nums\n",
    "        self.latent_dims = latent_dims\n",
    "\n",
    "        # Embedding layer\n",
    "        self.feature_embedding = nn.Embedding(feature_nums, latent_dims, padding_idx=0)\n",
    "        nn.init.xavier_uniform_(self.feature_embedding.weight.data)\n",
    "\n",
    "        # MLP\n",
    "        deep_input_dims = field_nums * latent_dims\n",
    "        layers = []\n",
    "\n",
    "        neuron_nums = [300, 300, 300]\n",
    "        for neuron_num in neuron_nums:\n",
    "            layers.append(nn.Linear(deep_input_dims, neuron_num))\n",
    "            # layers.append(nn.BatchNorm1d(neuron_num))  # Uncomment if needed\n",
    "            layers.append(nn.ReLU())\n",
    "            layers.append(nn.Dropout(0.2))\n",
    "            deep_input_dims = neuron_num\n",
    "\n",
    "        layers.append(nn.Linear(deep_input_dims, 1))\n",
    "\n",
    "        # Initialize weights\n",
    "        weight_init(layers)\n",
    "\n",
    "        self.mlp = nn.Sequential(*layers)\n",
    "\n",
    "    def forward(self, x):\n",
    "        embedding_x = self.feature_embedding(x)\n",
    "        out = self.mlp(embedding_x.view(-1, self.field_nums * self.latent_dims))\n",
    "        return out  # Return logits\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "DCN Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "metadata": {},
   "outputs": [],
   "source": [
    "class DCN(nn.Module):\n",
    "    def __init__(self, feature_nums, field_nums, latent_dims, output_dim=1):\n",
    "        super(DCN, self).__init__()\n",
    "        self.field_nums = field_nums\n",
    "        self.latent_dims = latent_dims\n",
    "\n",
    "        # Embedding layer\n",
    "        self.feature_embedding = nn.Embedding(feature_nums, latent_dims, padding_idx=0)\n",
    "        nn.init.xavier_uniform_(self.feature_embedding.weight.data)\n",
    "\n",
    "        # Deep Network\n",
    "        deep_input_dims = field_nums * latent_dims\n",
    "        deep_net_layers = []\n",
    "        neural_nums = [300, 300, 300]\n",
    "        self.num_neural_layers = 5  # Number of layers in the cross network\n",
    "\n",
    "        for neural_num in neural_nums:\n",
    "            deep_net_layers.append(nn.Linear(deep_input_dims, neural_num))\n",
    "            # deep_net_layers.append(nn.BatchNorm1d(neural_num))  # Uncomment if needed\n",
    "            deep_net_layers.append(nn.ReLU())\n",
    "            deep_net_layers.append(nn.Dropout(0.2))\n",
    "            deep_input_dims = neural_num\n",
    "\n",
    "        # Initialize weights\n",
    "        weight_init(deep_net_layers)\n",
    "\n",
    "        self.DN = nn.Sequential(*deep_net_layers)\n",
    "\n",
    "        # Cross Network\n",
    "        cross_input_dims = field_nums * latent_dims\n",
    "        self.cross_net_w = nn.ModuleList([\n",
    "            nn.Linear(cross_input_dims, cross_input_dims) for _ in range(self.num_neural_layers)\n",
    "        ])\n",
    "\n",
    "        # Initialize weights for cross network\n",
    "        weight_init(self.cross_net_w)\n",
    "\n",
    "        self.cross_net_b = nn.ParameterList([\n",
    "            nn.Parameter(torch.zeros(cross_input_dims)) for _ in range(self.num_neural_layers)\n",
    "        ])\n",
    "\n",
    "        # Final Linear Layer\n",
    "        self.linear = nn.Linear(deep_input_dims + cross_input_dims, output_dim)\n",
    "        # nn.init.xavier_normal_(self.linear.weight)\n",
    "\n",
    "    def forward(self, x):\n",
    "        embedding_x = self.feature_embedding(x).view(-1, self.field_nums * self.latent_dims)\n",
    "\n",
    "        # Cross Network\n",
    "        cn_x0 = embedding_x\n",
    "        cn_x = embedding_x\n",
    "        for i in range(self.num_neural_layers):\n",
    "            cn_x_w = self.cross_net_w[i](cn_x)\n",
    "            cn_x = cn_x0 * cn_x_w + self.cross_net_b[i] + cn_x\n",
    "\n",
    "        # Deep Network\n",
    "        dn_x = self.DN(embedding_x)\n",
    "\n",
    "        # Concatenate\n",
    "        x_stack = torch.cat([cn_x, dn_x], dim=1)\n",
    "\n",
    "        # Final output\n",
    "        out = self.linear(x_stack)\n",
    "\n",
    "        return out  # Return logits"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "AFM Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "metadata": {},
   "outputs": [],
   "source": [
    "class AFM(nn.Module):\n",
    "    def __init__(self, feature_nums, field_nums, latent_dims, output_dim=1):\n",
    "        super(AFM, self).__init__()\n",
    "        self.field_nums = field_nums\n",
    "        self.latent_dims = latent_dims\n",
    "\n",
    "        # Embedding layer\n",
    "        self.feature_embedding = nn.Embedding(feature_nums, latent_dims, padding_idx=0)\n",
    "        nn.init.xavier_uniform_(self.feature_embedding.weight.data)\n",
    "\n",
    "        # Prepare index pairs for interactions\n",
    "        self.row, self.col = [], []\n",
    "        for i in range(self.field_nums - 1):\n",
    "            for j in range(i + 1, self.field_nums):\n",
    "                self.row.append(i)\n",
    "                self.col.append(j)\n",
    "\n",
    "        attention_factor = self.latent_dims\n",
    "\n",
    "        # Attention network\n",
    "        self.attention_net = nn.Linear(self.latent_dims, attention_factor)\n",
    "        n = self.attention_net.in_features\n",
    "        y = 1.0 / np.sqrt(n)\n",
    "        self.attention_net.weight.data.uniform_(-y, y)\n",
    "        self.attention_net.bias.data.fill_(0)\n",
    "\n",
    "        self.attention_softmax = nn.Linear(attention_factor, 1)\n",
    "\n",
    "        # Output layers\n",
    "        self.fc = nn.Linear(self.latent_dims, output_dim)\n",
    "\n",
    "        # Linear part\n",
    "        self.linear = nn.Embedding(feature_nums, output_dim, padding_idx=0)\n",
    "        self.bias = nn.Parameter(torch.zeros((output_dim,)))\n",
    "\n",
    "    def forward(self, x):\n",
    "        embedding_x = self.feature_embedding(x)\n",
    "\n",
    "        # Pairwise interactions\n",
    "        row_emb = embedding_x[:, self.row]  # Shape: (batch_size, num_pairs, latent_dims)\n",
    "        col_emb = embedding_x[:, self.col]  # Shape: (batch_size, num_pairs, latent_dims)\n",
    "        inner_product = row_emb * col_emb  # Element-wise multiplication\n",
    "\n",
    "        # Attention mechanism\n",
    "        attn_scores = F.relu(self.attention_net(inner_product))  # Shape: (batch_size, num_pairs, attention_factor)\n",
    "        attn_scores = F.softmax(self.attention_softmax(attn_scores), dim=1)  # Shape: (batch_size, num_pairs, 1)\n",
    "        attn_scores = F.dropout(attn_scores, p=0.2)\n",
    "\n",
    "        # Weighted sum of interactions\n",
    "        attn_output = torch.sum(attn_scores * inner_product, dim=1)  # Shape: (batch_size, latent_dims)\n",
    "        attn_output = F.dropout(attn_output, p=0.2)\n",
    "\n",
    "        # Output\n",
    "        linear_part = self.bias + torch.sum(self.linear(x), dim=1)  # Shape: (batch_size, output_dim)\n",
    "        out = linear_part + self.fc(attn_output)  # Shape: (batch_size, output_dim)\n",
    "\n",
    "        return out  # Return logits"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Training and Testing with CoTTA\n",
    "### Main Function"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "metadata": {},
   "outputs": [],
   "source": [
    "def main():\n",
    "    data_path = '/home/vladplyusnin/tftest/Deep-Learning-COPSCI764/Project/make-ipinyou-data/cikm2016-yoyi-dataset/'  # Adjust this path as needed\n",
    "\n",
    "    train_file = os.path.join(data_path, 'train_set.txt')\n",
    "    test_file = os.path.join(data_path, 'test_set.txt')\n",
    "\n",
    "    # Build feature mapping and get feature_nums\n",
    "    print(\"Building feature mapping...\")\n",
    "    feature_id_map, feature_nums = build_feature_mapping([train_file, test_file])\n",
    "    print(f\"Total number of features: {feature_nums}\")\n",
    "\n",
    "    # Compute max_length\n",
    "    print(\"Computing maximum feature length...\")\n",
    "    max_length = compute_max_length([train_file, test_file])\n",
    "    print(f\"Maximum feature length: {max_length}\")\n",
    "\n",
    "    field_nums = max_length  # Since we've padded features to max_length\n",
    "\n",
    "    batch_size = 1024\n",
    "    num_workers = 8  # Adjust based on your system\n",
    "\n",
    "    # Create data loaders\n",
    "    print(\"Creating data loaders...\")\n",
    "    train_loader = create_data_loader(train_file, max_length, batch_size, num_workers, feature_id_map)\n",
    "    test_loader = create_data_loader(test_file, max_length, batch_size, num_workers, feature_id_map)\n",
    "\n",
    "    # Since we cannot split the data for validation easily, we'll sample a small subset for validation\n",
    "    # For demonstration, we'll use the first N batches as validation\n",
    "    val_loader = None  # Set to None if not using validation\n",
    "\n",
    "    # Model parameters\n",
    "    model_name = 'DeepFM'  # Change this to 'FNN', 'DCN', or 'AFM' as needed\n",
    "    latent_dims = 10\n",
    "    dropout = 0.2\n",
    "    num_layers = 5  # For DCN\n",
    "    attn_size = 32  # For AFM\n",
    "    device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
    "    epochs = 3  # Adjust as needed\n",
    "\n",
    "    # Get model\n",
    "    print(f\"Initializing model: {model_name}\")\n",
    "    if model_name == 'DeepFM':\n",
    "        model = DeepFM(feature_nums, field_nums, latent_dims).to(device)\n",
    "    elif model_name == 'FNN':\n",
    "        model = FNN(feature_nums, field_nums, latent_dims).to(device)\n",
    "    elif model_name == 'DCN':\n",
    "        model = DCN(feature_nums, field_nums, latent_dims).to(device)\n",
    "    elif model_name == 'AFM':\n",
    "        model = AFM(feature_nums, field_nums, latent_dims).to(device)\n",
    "    else:\n",
    "        raise ValueError('Unknown model name')\n",
    "\n",
    "    # Loss and optimizer\n",
    "    criterion = nn.BCEWithLogitsLoss()\n",
    "    optimizer = optim.Adam(model.parameters(), lr=1e-4, weight_decay=1e-5)\n",
    "\n",
    "    # Training with early stopping\n",
    "    print(\"Starting training...\")\n",
    "    train_model(model, train_loader, val_loader, criterion, optimizer, device, epochs, early_stopping_patience=2)\n",
    "\n",
    "    # Load best model\n",
    "    model.load_state_dict(torch.load('best_model.pth'))\n",
    "\n",
    "    # Configure model for CoTTA\n",
    "    model = configure_model(model)\n",
    "    params, param_names = collect_params(model)\n",
    "    optimizer = torch.optim.SGD(params, lr=0.0001)\n",
    "    cotta_model = CoTTA(model, optimizer, steps=1, episodic=False)\n",
    "\n",
    "    # Testing with CoTTA\n",
    "    print(\"Starting testing with CoTTA...\")\n",
    "    model.eval()\n",
    "    y_true = []\n",
    "    y_scores = []\n",
    "\n",
    "    for x_batch, y_batch in test_loader:\n",
    "        x_batch = x_batch.to(device)\n",
    "        y_batch = y_batch.to(device)\n",
    "\n",
    "        outputs = cotta_model(x_batch)  # Outputs are logits\n",
    "        probabilities = torch.sigmoid(outputs).detach().cpu().numpy().flatten()\n",
    "\n",
    "        y_true.extend(y_batch.cpu().numpy())\n",
    "        y_scores.extend(probabilities)\n",
    "\n",
    "    test_auc = roc_auc_score(y_true, y_scores)\n",
    "    print(f'Test AUC with CoTTA: {test_auc:.4f}')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Training Function"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "metadata": {},
   "outputs": [],
   "source": [
    "def train_model(model, train_loader, valid_loader, criterion, optimizer, device, epochs, early_stopping_patience=2):\n",
    "    best_loss = float('inf')\n",
    "    epochs_no_improve = 0\n",
    "    early_stop = False\n",
    "\n",
    "    for epoch in range(epochs):\n",
    "        if early_stop:\n",
    "            print(\"Early stopping\")\n",
    "            break\n",
    "\n",
    "        model.train()\n",
    "        total_loss = 0\n",
    "        num_batches = 0  # Keep track of the number of batches\n",
    "        for x_batch, y_batch in train_loader:\n",
    "            x_batch = x_batch.to(device)\n",
    "            y_batch = y_batch.to(device)\n",
    "            optimizer.zero_grad()\n",
    "            logits = model(x_batch).squeeze()\n",
    "            loss = criterion(logits, y_batch)\n",
    "            loss.backward()\n",
    "            optimizer.step()\n",
    "            total_loss += loss.item()\n",
    "            num_batches += 1\n",
    "        avg_loss = total_loss / num_batches if num_batches > 0 else 0\n",
    "        print(f'Epoch {epoch+1}/{epochs}, Loss: {avg_loss:.4f}')\n",
    "\n",
    "        # Validation\n",
    "        if valid_loader is not None:\n",
    "            model.eval()\n",
    "            val_total_loss = 0\n",
    "            num_val_batches = 0  # Keep track of the number of validation batches\n",
    "            y_true = []\n",
    "            y_scores = []\n",
    "            with torch.no_grad():\n",
    "                for x_val, y_val in valid_loader:\n",
    "                    x_val = x_val.to(device)\n",
    "                    y_val = y_val.to(device)\n",
    "                    logits = model(x_val).squeeze()\n",
    "                    loss = criterion(logits, y_val)\n",
    "                    val_total_loss += loss.item()\n",
    "                    num_val_batches += 1\n",
    "                    y_pred = torch.sigmoid(logits)\n",
    "                    y_true.extend(y_val.cpu().numpy())\n",
    "                    y_scores.extend(y_pred.cpu().numpy())\n",
    "            val_avg_loss = val_total_loss / num_val_batches if num_val_batches > 0 else 0\n",
    "            val_auc = roc_auc_score(y_true, y_scores)\n",
    "            print(f'Val Loss: {val_avg_loss:.4f}, Validation AUC: {val_auc:.4f}')\n",
    "\n",
    "            # Check for early stopping\n",
    "            if val_avg_loss < best_loss:\n",
    "                best_loss = val_avg_loss\n",
    "                epochs_no_improve = 0\n",
    "                # Save best model\n",
    "                torch.save(model.state_dict(), 'best_model.pth')\n",
    "            else:\n",
    "                epochs_no_improve += 1\n",
    "                if epochs_no_improve >= early_stopping_patience:\n",
    "                    print(f\"Early stopping after {epoch+1} epochs\")\n",
    "                    early_stop = True\n",
    "        else:\n",
    "            # Save model every epoch if no validation set\n",
    "            torch.save(model.state_dict(), 'best_model.pth')\n",
    "\n",
    "    if valid_loader is not None:\n",
    "        print(f'Best Validation Loss: {best_loss:.4f}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Building feature mapping...\n",
      "Total number of features: 1782232\n",
      "Computing maximum feature length...\n",
      "Maximum feature length: 37\n",
      "Creating data loaders...\n",
      "Initializing model: DeepFM\n",
      "Starting training...\n",
      "Epoch 1/3, Loss: 0.0085\n",
      "Epoch 2/3, Loss: 0.0082\n",
      "Epoch 3/3, Loss: 0.0082\n",
      "Parameter to adapt: linear.weight\n",
      "Parameter to adapt: feature_embedding.weight\n",
      "Parameter to adapt: linear.weight\n",
      "Parameter to adapt: feature_embedding.weight\n",
      "Starting testing with CoTTA...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/vladplyusnin/.local/lib/python3.10/site-packages/torch/_compile.py:24: UserWarning: optimizer contains a parameter group with duplicate parameters; in future, this will cause an error; see github.com/pytorch/pytorch/issues/40967 for more information\n",
      "  return torch._dynamo.disable(fn, recursive)(*args, **kwargs)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Test AUC with CoTTA: 0.8723\n"
     ]
    }
   ],
   "source": [
    "if __name__ == '__main__':\n",
    "    main()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
