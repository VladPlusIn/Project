{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 109,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import copy\n",
    "from copy import deepcopy\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "import torch.utils.data\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "from sklearn.metrics import roc_auc_score\n",
    "import torch.nn.functional as F"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Transformation Function"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 110,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_tta_transforms():\n",
    "    # Define a transformation that randomly masks some features\n",
    "    class RandomFeatureMasking:\n",
    "        def __init__(self, p=0.2):\n",
    "            self.p = p\n",
    "\n",
    "        def __call__(self, x):\n",
    "            # x is a tensor of shape (batch_size, num_features)\n",
    "            # Generate a mask with probability p of masking each feature\n",
    "            mask = (torch.rand_like(x.float()) > self.p).long()\n",
    "            x_transformed = x * mask\n",
    "            return x_transformed\n",
    "\n",
    "    return RandomFeatureMasking(p=0.2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 111,
   "metadata": {},
   "outputs": [],
   "source": [
    "def weight_init(layers):\n",
    "    for layer in layers:\n",
    "        if isinstance(layer, nn.BatchNorm1d):\n",
    "            layer.weight.data.fill_(1)\n",
    "            layer.bias.data.zero_()\n",
    "        elif isinstance(layer, nn.Linear):\n",
    "            n = layer.in_features\n",
    "            y = 1.0 / np.sqrt(n)\n",
    "            layer.weight.data.uniform_(-y, y)\n",
    "            if layer.bias is not None:\n",
    "                layer.bias.data.fill_(0)\n",
    "            # nn.init.kaiming_normal_(layer.weight.data, nonlinearity='relu')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "CoTTA Utility Functions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 112,
   "metadata": {},
   "outputs": [],
   "source": [
    "def copy_model_and_optimizer(model, optimizer):\n",
    "    \"\"\"Copy the model and optimizer states for resetting after adaptation.\"\"\"\n",
    "    model_state = deepcopy(model.state_dict())\n",
    "    model_anchor = deepcopy(model)\n",
    "    optimizer_state = deepcopy(optimizer.state_dict())\n",
    "    ema_model = deepcopy(model)\n",
    "    for param in ema_model.parameters():\n",
    "        param.detach_()\n",
    "    return model_state, optimizer_state, ema_model, model_anchor\n",
    "\n",
    "def load_model_and_optimizer(model, optimizer, model_state, optimizer_state):\n",
    "    \"\"\"Restore the model and optimizer states from copies.\"\"\"\n",
    "    model.load_state_dict(model_state, strict=True)\n",
    "    optimizer.load_state_dict(optimizer_state)\n",
    "\n",
    "def configure_model(model):\n",
    "    \"\"\"Configure model for use with CoTTA.\"\"\"\n",
    "    model.train()\n",
    "    model.requires_grad_(False)\n",
    "    for m in model.modules():\n",
    "        if isinstance(m, (nn.Embedding, nn.BatchNorm1d)):\n",
    "            m.requires_grad_(True)\n",
    "            if isinstance(m, nn.BatchNorm1d):\n",
    "                m.track_running_stats = False\n",
    "                m.running_mean = None\n",
    "                m.running_var = None\n",
    "    return model\n",
    "\n",
    "def collect_params(model):\n",
    "    \"\"\"Collect all trainable parameters.\"\"\"\n",
    "    params = []\n",
    "    names = []\n",
    "    for nm, m in model.named_modules():\n",
    "        for np_, p in m.named_parameters():\n",
    "            if p.requires_grad:\n",
    "                params.append(p)\n",
    "                key = f\"{nm}.{np_}\" if nm else np_\n",
    "                names.append(key)\n",
    "    return params, names"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Custom Binary Entropy Function"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 113,
   "metadata": {},
   "outputs": [],
   "source": [
    "def binary_entropy(logits, logits_ema):\n",
    "    \"\"\"Entropy for binary classification.\"\"\"\n",
    "    p = torch.sigmoid(logits)\n",
    "    p_ema = torch.sigmoid(logits_ema)\n",
    "    entropy = -0.5 * (p_ema * torch.log(p + 1e-8) + (1 - p_ema) * torch.log(1 - p + 1e-8))\n",
    "    entropy -= 0.5 * (p * torch.log(p_ema + 1e-8) + (1 - p) * torch.log(1 - p_ema + 1e-8))\n",
    "    return entropy.squeeze()\n",
    "\n",
    "def update_ema_variables(ema_model, model, alpha_teacher):\n",
    "    \"\"\"Update EMA model parameters.\"\"\"\n",
    "    for ema_param, param in zip(ema_model.parameters(), model.parameters()):\n",
    "        ema_param.data.mul_(alpha_teacher).add_(param.data * (1 - alpha_teacher))\n",
    "    return ema_model"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "CoTTA Class"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 114,
   "metadata": {},
   "outputs": [],
   "source": [
    "class CoTTA(nn.Module):\n",
    "    \"\"\"CoTTA adapts a model by entropy minimization during testing.\"\"\"\n",
    "    def __init__(self, model, optimizer, steps=1, episodic=False):\n",
    "        super().__init__()\n",
    "        self.model = model\n",
    "        self.optimizer = optimizer\n",
    "        self.steps = steps\n",
    "        assert steps > 0, \"CoTTA requires >= 1 step(s) to forward and update\"\n",
    "        self.episodic = episodic\n",
    "\n",
    "        self.model_state, self.optimizer_state, self.model_ema, self.model_anchor = \\\n",
    "            copy_model_and_optimizer(self.model, self.optimizer)\n",
    "        self.transform = get_tta_transforms()\n",
    "\n",
    "    def forward(self, x):\n",
    "        if self.episodic:\n",
    "            self.reset()\n",
    "        for _ in range(self.steps):\n",
    "            outputs = self.forward_and_adapt(x, self.model, self.optimizer)\n",
    "        return outputs\n",
    "\n",
    "    def reset(self):\n",
    "        if self.model_state is None or self.optimizer_state is None:\n",
    "            raise Exception(\"Cannot reset without saved model/optimizer state\")\n",
    "        load_model_and_optimizer(self.model, self.optimizer,\n",
    "                                 self.model_state, self.optimizer_state)\n",
    "        self.model_state, self.optimizer_state, self.model_ema, self.model_anchor = \\\n",
    "            copy_model_and_optimizer(self.model, self.optimizer)\n",
    "\n",
    "    @torch.enable_grad()\n",
    "    def forward_and_adapt(self, x, model, optimizer):\n",
    "        outputs = self.model(x)\n",
    "        self.model_ema.train()\n",
    "        # Teacher Prediction\n",
    "        anchor_prob = torch.sigmoid(self.model_anchor(x)).detach()\n",
    "        standard_ema = self.model_ema(x)\n",
    "        # Augmentation-averaged Prediction\n",
    "        N = 32\n",
    "        outputs_emas = []\n",
    "        to_aug = anchor_prob.mean() < 0.1  # Adjusted for binary classification\n",
    "        if to_aug:\n",
    "            for i in range(N):\n",
    "                outputs_ = self.model_ema(self.transform(x)).detach()\n",
    "                outputs_emas.append(outputs_)\n",
    "            outputs_ema = torch.stack(outputs_emas).mean(0)\n",
    "        else:\n",
    "            outputs_ema = standard_ema\n",
    "        # Student update\n",
    "        loss = binary_entropy(outputs, outputs_ema.detach()).mean()\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "        optimizer.zero_grad()\n",
    "        # Teacher update\n",
    "        self.model_ema = update_ema_variables(self.model_ema, self.model, alpha_teacher=0.999)\n",
    "        # Stochastic restore\n",
    "        for nm, m in self.model.named_modules():\n",
    "            for np_, p in m.named_parameters():\n",
    "                if p.requires_grad:\n",
    "                    mask = (torch.rand(p.shape) < 0.001).float().to(p.device)\n",
    "                    with torch.no_grad():\n",
    "                        key = f\"{nm}.{np_}\" if nm else np_\n",
    "                        p.data = self.model_state[key] * mask + p * (1. - mask)\n",
    "        return outputs_ema"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Data Loading and Processing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 115,
   "metadata": {},
   "outputs": [],
   "source": [
    "def load_data(data_path):\n",
    "    # Read feature mapping\n",
    "    with open(os.path.join(data_path, 'feat.bid.txt'), 'r') as f:\n",
    "        lines = f.readlines()\n",
    "    feature_nums = int(lines[0].strip())\n",
    "    # Assuming features are already mapped to integers in the data files\n",
    "\n",
    "    # Read train data\n",
    "    train_data = pd.read_csv(os.path.join(data_path, 'train.bid.txt'), header=None)\n",
    "    # Read test data\n",
    "    test_data = pd.read_csv(os.path.join(data_path, 'test.bid.txt'), header=None)\n",
    "\n",
    "    return train_data, test_data, feature_nums"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Dataset Class"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 116,
   "metadata": {},
   "outputs": [],
   "source": [
    "class CTRDataset(Dataset):\n",
    "    def __init__(self, data):\n",
    "        # Data columns: click + winning price + hour + time_fraction + timestamp + features\n",
    "        self.labels = data.iloc[:, 0].values.astype(np.float32)  # click labels\n",
    "        self.features = data.iloc[:, 5:].values.astype(np.int64)  # features start from column 5\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.labels)\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        x = self.features[idx]\n",
    "        y = self.labels[idx]\n",
    "        return torch.tensor(x, dtype=torch.long), torch.tensor(y, dtype=torch.float32)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Function to expand embedding layers"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 117,
   "metadata": {},
   "outputs": [],
   "source": [
    "def expand_embedding_layer(old_embedding, new_num_embeddings):\n",
    "    old_num_embeddings, embedding_dim = old_embedding.weight.size()\n",
    "    if new_num_embeddings <= old_num_embeddings:\n",
    "        return old_embedding  # No need to expand\n",
    "    # Create new embedding layer\n",
    "    new_embedding = nn.Embedding(new_num_embeddings, embedding_dim)\n",
    "    # Copy weights from old embedding\n",
    "    with torch.no_grad():\n",
    "        new_embedding.weight[:old_num_embeddings] = old_embedding.weight\n",
    "        # Initialize new embeddings\n",
    "        nn.init.xavier_uniform_(new_embedding.weight[old_num_embeddings:])\n",
    "    return new_embedding"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Model Implementations\n",
    "### DeepFM Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 118,
   "metadata": {},
   "outputs": [],
   "source": [
    "class DeepFM(nn.Module):\n",
    "    def __init__(self,\n",
    "                 feature_nums,\n",
    "                 field_nums,\n",
    "                 latent_dims,\n",
    "                 output_dim=1):\n",
    "        super(DeepFM, self).__init__()\n",
    "        self.feature_nums = feature_nums\n",
    "        self.field_nums = field_nums\n",
    "        self.latent_dims = latent_dims\n",
    "\n",
    "        # Linear part\n",
    "        self.linear = nn.Embedding(self.feature_nums, output_dim)\n",
    "\n",
    "        # FM embedding\n",
    "        self.feature_embedding = nn.Embedding(self.feature_nums, self.latent_dims)\n",
    "        nn.init.xavier_uniform_(self.feature_embedding.weight.data)\n",
    "\n",
    "        # MLP\n",
    "        deep_input_dims = self.field_nums * self.latent_dims\n",
    "        layers = []\n",
    "\n",
    "        neuron_nums = [300, 300, 300]\n",
    "        for neuron_num in neuron_nums:\n",
    "            layers.append(nn.Linear(deep_input_dims, neuron_num))\n",
    "            # layers.append(nn.BatchNorm1d(neuron_num))  # Uncomment if needed\n",
    "            layers.append(nn.ReLU())\n",
    "            layers.append(nn.Dropout(p=0.2))\n",
    "            deep_input_dims = neuron_num\n",
    "\n",
    "        layers.append(nn.Linear(deep_input_dims, 1))\n",
    "\n",
    "        # Initialize weights\n",
    "        weight_init(layers)\n",
    "\n",
    "        self.mlp = nn.Sequential(*layers)\n",
    "\n",
    "    def expand_embeddings(self, new_feature_nums):\n",
    "        self.linear = expand_embedding_layer(self.linear, new_feature_nums)\n",
    "        self.feature_embedding = expand_embedding_layer(self.feature_embedding, new_feature_nums)\n",
    "        self.feature_nums = new_feature_nums\n",
    "\n",
    "    def forward(self, x):\n",
    "        # Linear Part\n",
    "        linear_out = torch.sum(self.linear(x), dim=1)  # Shape: (batch_size, output_dim)\n",
    "\n",
    "        # FM Part\n",
    "        embedding_x = self.feature_embedding(x)\n",
    "        square_of_sum = torch.sum(embedding_x, dim=1) ** 2\n",
    "        sum_of_square = torch.sum(embedding_x ** 2, dim=1)\n",
    "        ix = 0.5 * torch.sum(square_of_sum - sum_of_square, dim=1, keepdim=True)  # Shape: (batch_size, 1)\n",
    "\n",
    "        # Deep Part\n",
    "        deep_out = self.mlp(embedding_x.view(-1, self.field_nums * self.latent_dims))  # Shape: (batch_size, 1)\n",
    "\n",
    "        # Output\n",
    "        out = linear_out + ix + deep_out  # Shape: (batch_size, 1)\n",
    "\n",
    "        return out  # Return logits"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "FNN Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 119,
   "metadata": {},
   "outputs": [],
   "source": [
    "class FNN(nn.Module):\n",
    "    def __init__(self,\n",
    "                 feature_nums,\n",
    "                 field_nums,\n",
    "                 latent_dims):\n",
    "        super(FNN, self).__init__()\n",
    "        self.feature_nums = feature_nums\n",
    "        self.field_nums = field_nums\n",
    "        self.latent_dims = latent_dims\n",
    "\n",
    "        # Embedding layer\n",
    "        self.feature_embedding = nn.Embedding(self.feature_nums, self.latent_dims)\n",
    "        nn.init.xavier_uniform_(self.feature_embedding.weight.data)\n",
    "\n",
    "        # MLP\n",
    "        deep_input_dims = self.field_nums * self.latent_dims\n",
    "        layers = []\n",
    "\n",
    "        neuron_nums = [300, 300, 300]\n",
    "        for neuron_num in neuron_nums:\n",
    "            layers.append(nn.Linear(deep_input_dims, neuron_num))\n",
    "            # layers.append(nn.BatchNorm1d(neuron_num))  # Uncomment if needed\n",
    "            layers.append(nn.ReLU())\n",
    "            layers.append(nn.Dropout(p=0.2))\n",
    "            deep_input_dims = neuron_num\n",
    "\n",
    "        layers.append(nn.Linear(deep_input_dims, 1))\n",
    "\n",
    "        # Initialize weights\n",
    "        weight_init(layers)\n",
    "\n",
    "        self.mlp = nn.Sequential(*layers)\n",
    "\n",
    "    def expand_embeddings(self, new_feature_nums):\n",
    "        self.feature_embedding = expand_embedding_layer(self.feature_embedding, new_feature_nums)\n",
    "        self.feature_nums = new_feature_nums\n",
    "\n",
    "    def forward(self, x):\n",
    "        embedding_x = self.feature_embedding(x)\n",
    "        out = self.mlp(embedding_x.view(-1, self.field_nums * self.latent_dims))\n",
    "\n",
    "        return out  # Return logits\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "DCN Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 120,
   "metadata": {},
   "outputs": [],
   "source": [
    "class DCN(nn.Module):\n",
    "    def __init__(self,\n",
    "                 feature_nums,\n",
    "                 field_nums,\n",
    "                 latent_dims,\n",
    "                 output_dim=1):\n",
    "        super(DCN, self).__init__()\n",
    "        self.feature_nums = feature_nums\n",
    "        self.field_nums = field_nums\n",
    "        self.latent_dims = latent_dims\n",
    "\n",
    "        # Embedding layer\n",
    "        self.feature_embedding = nn.Embedding(self.feature_nums, self.latent_dims)\n",
    "        nn.init.xavier_uniform_(self.feature_embedding.weight.data)\n",
    "\n",
    "        # Deep Network\n",
    "        deep_input_dims = self.field_nums * self.latent_dims\n",
    "        deep_net_layers = []\n",
    "        neural_nums = [300, 300, 300]\n",
    "        self.num_neural_layers = 5  # Number of layers in the cross network\n",
    "\n",
    "        for neural_num in neural_nums:\n",
    "            deep_net_layers.append(nn.Linear(deep_input_dims, neural_num))\n",
    "            # deep_net_layers.append(nn.BatchNorm1d(neural_num))  # Uncomment if needed\n",
    "            deep_net_layers.append(nn.ReLU())\n",
    "            deep_net_layers.append(nn.Dropout(p=0.2))\n",
    "            deep_input_dims = neural_num\n",
    "\n",
    "        # Initialize weights\n",
    "        weight_init(deep_net_layers)\n",
    "\n",
    "        self.DN = nn.Sequential(*deep_net_layers)\n",
    "\n",
    "        # Cross Network\n",
    "        cross_input_dims = self.field_nums * self.latent_dims\n",
    "        self.cross_net_w = nn.ModuleList([\n",
    "            nn.Linear(cross_input_dims, cross_input_dims) for _ in range(self.num_neural_layers)\n",
    "        ])\n",
    "\n",
    "        # Initialize weights for cross network\n",
    "        weight_init(self.cross_net_w)\n",
    "\n",
    "        self.cross_net_b = nn.ParameterList([\n",
    "            nn.Parameter(torch.zeros(cross_input_dims)) for _ in range(self.num_neural_layers)\n",
    "        ])\n",
    "\n",
    "        # Final Linear Layer\n",
    "        self.linear = nn.Linear(deep_input_dims + cross_input_dims, output_dim)\n",
    "        # nn.init.xavier_normal_(self.linear.weight)\n",
    "\n",
    "    def expand_embeddings(self, new_feature_nums):\n",
    "        self.feature_embedding = expand_embedding_layer(self.feature_embedding, new_feature_nums)\n",
    "        self.feature_nums = new_feature_nums\n",
    "        # Adjust input dimensions if latent_dims remain the same\n",
    "        cross_input_dims = self.field_nums * self.latent_dims\n",
    "        for layer in self.cross_net_w:\n",
    "            layer.in_features = cross_input_dims\n",
    "            layer.out_features = cross_input_dims\n",
    "            layer.weight.data = torch.randn(cross_input_dims, cross_input_dims) * np.sqrt(2.0 / cross_input_dims)\n",
    "        for i in range(len(self.cross_net_b)):\n",
    "            self.cross_net_b[i] = nn.Parameter(torch.zeros(cross_input_dims))\n",
    "        self.linear = nn.Linear(cross_input_dims + self.DN[-2].out_features, 1)\n",
    "\n",
    "    def forward(self, x):\n",
    "        embedding_x = self.feature_embedding(x).view(-1, self.field_nums * self.latent_dims)\n",
    "\n",
    "        # Cross Network\n",
    "        cn_x0 = embedding_x\n",
    "        cn_x = embedding_x\n",
    "        for i in range(self.num_neural_layers):\n",
    "            cn_x_w = self.cross_net_w[i](cn_x)\n",
    "            cn_x = cn_x0 * cn_x_w + self.cross_net_b[i] + cn_x\n",
    "\n",
    "        # Deep Network\n",
    "        dn_x = self.DN(embedding_x)\n",
    "\n",
    "        # Concatenate\n",
    "        x_stack = torch.cat([cn_x, dn_x], dim=1)\n",
    "\n",
    "        # Final output\n",
    "        out = self.linear(x_stack)\n",
    "\n",
    "        return out  # Return logits"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "AFM Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 121,
   "metadata": {},
   "outputs": [],
   "source": [
    "class AFM(nn.Module):\n",
    "    def __init__(self,\n",
    "                 feature_nums,\n",
    "                 field_nums,\n",
    "                 latent_dims,\n",
    "                 output_dim=1):\n",
    "        super(AFM, self).__init__()\n",
    "        self.feature_nums = feature_nums\n",
    "        self.field_nums = field_nums\n",
    "        self.latent_dims = latent_dims\n",
    "\n",
    "        # Embedding layer\n",
    "        self.feature_embedding = nn.Embedding(self.feature_nums, self.latent_dims)\n",
    "        nn.init.xavier_uniform_(self.feature_embedding.weight.data)\n",
    "\n",
    "        # Prepare index pairs for interactions\n",
    "        self.row, self.col = [], []\n",
    "        for i in range(self.field_nums - 1):\n",
    "            for j in range(i + 1, self.field_nums):\n",
    "                self.row.append(i)\n",
    "                self.col.append(j)\n",
    "\n",
    "        attention_factor = self.latent_dims\n",
    "\n",
    "        # Attention network\n",
    "        self.attention_net = nn.Linear(self.latent_dims, attention_factor)\n",
    "        n = self.attention_net.in_features\n",
    "        y = 1.0 / np.sqrt(n)\n",
    "        self.attention_net.weight.data.uniform_(-y, y)\n",
    "        self.attention_net.bias.data.fill_(0)\n",
    "\n",
    "        self.attention_softmax = nn.Linear(attention_factor, 1)\n",
    "\n",
    "        # Output layers\n",
    "        self.fc = nn.Linear(self.latent_dims, output_dim)\n",
    "\n",
    "        # Linear part\n",
    "        self.linear = nn.Embedding(self.feature_nums, output_dim)\n",
    "        self.bias = nn.Parameter(torch.zeros((output_dim,)))\n",
    "\n",
    "    def expand_embeddings(self, new_feature_nums):\n",
    "        self.feature_embedding = expand_embedding_layer(self.feature_embedding, new_feature_nums)\n",
    "        self.linear = expand_embedding_layer(self.linear, new_feature_nums)\n",
    "        self.feature_nums = new_feature_nums\n",
    "\n",
    "    def forward(self, x):\n",
    "        embedding_x = self.feature_embedding(x)\n",
    "\n",
    "        # Pairwise interactions\n",
    "        row_emb = embedding_x[:, self.row]  # Shape: (batch_size, num_pairs, latent_dims)\n",
    "        col_emb = embedding_x[:, self.col]  # Shape: (batch_size, num_pairs, latent_dims)\n",
    "        inner_product = row_emb * col_emb  # Element-wise multiplication\n",
    "\n",
    "        # Attention mechanism\n",
    "        attn_scores = F.relu(self.attention_net(inner_product))  # Shape: (batch_size, num_pairs, attention_factor)\n",
    "        attn_scores = F.softmax(self.attention_softmax(attn_scores), dim=1)  # Shape: (batch_size, num_pairs, 1)\n",
    "        attn_scores = F.dropout(attn_scores, p=0.2)\n",
    "\n",
    "        # Weighted sum of interactions\n",
    "        attn_output = torch.sum(attn_scores * inner_product, dim=1)  # Shape: (batch_size, latent_dims)\n",
    "        attn_output = F.dropout(attn_output, p=0.2)\n",
    "\n",
    "        # Output\n",
    "        linear_part = self.bias + torch.sum(self.linear(x), dim=1)  # Shape: (batch_size, output_dim)\n",
    "        out = linear_part + self.fc(attn_output)  # Shape: (batch_size, output_dim)\n",
    "\n",
    "        return out  # Return logits"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Training and Testing with CoTTA\n",
    "### Main Function"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 122,
   "metadata": {},
   "outputs": [],
   "source": [
    "def main():\n",
    "    dataset_ids = ['1458', '2259', '2261', '2821', '2997', '3358', '3386', '3427', '3476']\n",
    "    device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
    "    batch_size = 1024\n",
    "\n",
    "    # Model parameters\n",
    "    model_name = 'AFM'  # Change as needed\n",
    "    latent_dims = 10\n",
    "    dropout = 0.2\n",
    "    num_layers = 5  # For DCN\n",
    "    attn_size = 32  # For AFM\n",
    "    epochs_per_dataset = 1  # Train for 1 epoch per dataset\n",
    "\n",
    "    # Paths\n",
    "    base_data_path = '/home/vladplyusnin/tftest/Deep-Learning-COPSCI764/Project/data/ipinyou'  # Adjust this path as needed\n",
    "\n",
    "    # Initialize the model (we need to know feature_nums and field_nums)\n",
    "    # For that, we need to load the first dataset to get feature_nums and field_nums\n",
    "\n",
    "    # Initialize variables\n",
    "    max_feature_nums = 0\n",
    "    field_nums = None  # Will be set after loading the first dataset\n",
    "\n",
    "    # Initialize model as None\n",
    "    model = None\n",
    "\n",
    "    # Loss and optimizer (initialized later)\n",
    "    criterion = nn.BCEWithLogitsLoss()\n",
    "    optimizer = None\n",
    "\n",
    "    for dataset_id in dataset_ids:\n",
    "        data_path = os.path.join(base_data_path, dataset_id)\n",
    "\n",
    "        # Load data\n",
    "        train_data, test_data, feature_nums_dataset = load_data(data_path)\n",
    "        field_nums_dataset = train_data.shape[1] - 5\n",
    "\n",
    "        # Update max_feature_nums if needed\n",
    "        if feature_nums_dataset > max_feature_nums:\n",
    "            max_feature_nums = feature_nums_dataset\n",
    "\n",
    "        # Initialize field_nums if not set\n",
    "        if field_nums is None:\n",
    "            field_nums = field_nums_dataset\n",
    "\n",
    "        # Check that field_nums are the same\n",
    "        if field_nums != field_nums_dataset:\n",
    "            print(f\"Dataset {dataset_id} has different field_nums.\")\n",
    "            # Handle this case as needed\n",
    "            continue  # Skip this dataset for now\n",
    "\n",
    "        # Expand model's embeddings if feature_nums increased\n",
    "        if model is None:\n",
    "            # Initialize the model\n",
    "            if model_name == 'DeepFM':\n",
    "                model = DeepFM(max_feature_nums, field_nums, latent_dims).to(device)\n",
    "            elif model_name == 'FNN':\n",
    "                model = FNN(max_feature_nums, field_nums, latent_dims).to(device)\n",
    "            elif model_name == 'DCN':\n",
    "                model = DCN(max_feature_nums, field_nums, latent_dims).to(device)\n",
    "            elif model_name == 'AFM':\n",
    "                model = AFM(max_feature_nums, field_nums, latent_dims).to(device)\n",
    "            else:\n",
    "                raise ValueError('Unknown model name')\n",
    "            # Initialize optimizer\n",
    "            optimizer = optim.Adam(model.parameters(), lr=1e-4, weight_decay=1e-5)\n",
    "        else:\n",
    "            # Expand embeddings if needed\n",
    "            if feature_nums_dataset > model.feature_nums:\n",
    "                model.expand_embeddings(feature_nums_dataset)\n",
    "                # Update optimizer with new parameters\n",
    "                optimizer = optim.Adam(model.parameters(), lr=1e-4, weight_decay=1e-5)\n",
    "\n",
    "        # Split train data into training and validation sets\n",
    "        from sklearn.model_selection import train_test_split\n",
    "        train_df, val_df = train_test_split(train_data, test_size=0.2, random_state=42)\n",
    "\n",
    "        # Create datasets and loaders\n",
    "        train_dataset = CTRDataset(train_df)\n",
    "        val_dataset = CTRDataset(val_df)\n",
    "        test_dataset = CTRDataset(test_data)\n",
    "\n",
    "        train_loader = DataLoader(train_dataset, batch_size=batch_size, shuffle=True, num_workers=4)\n",
    "        val_loader = DataLoader(val_dataset, batch_size=batch_size, num_workers=4)\n",
    "        test_loader = DataLoader(test_dataset, batch_size=batch_size, num_workers=4)\n",
    "\n",
    "        print(f\"\\nTraining on dataset {dataset_id}\")\n",
    "        # Train the model for 1 epoch on the current dataset\n",
    "        train_model(model, train_loader, val_loader, criterion, optimizer, device, epochs=1, early_stopping_patience=None)\n",
    "\n",
    "        # After training, re-initialize CoTTA with the updated model\n",
    "        # Configure model for CoTTA\n",
    "        cotta_model = deepcopy(model)\n",
    "        cotta_model = configure_model(cotta_model)\n",
    "        params, param_names = collect_params(cotta_model)\n",
    "        cotta_optimizer = torch.optim.SGD(params, lr=0.0001)\n",
    "        cotta = CoTTA(cotta_model, cotta_optimizer, steps=1, episodic=False)\n",
    "\n",
    "        # Testing with CoTTA\n",
    "        cotta_model.eval()\n",
    "        y_true = []\n",
    "        y_scores = []\n",
    "\n",
    "        for x_batch, y_batch in test_loader:\n",
    "            x_batch = x_batch.to(device)\n",
    "            y_batch = y_batch.to(device)\n",
    "\n",
    "            outputs = cotta(x_batch)  # Outputs are logits\n",
    "            probabilities = torch.sigmoid(outputs).detach().cpu().numpy().flatten()\n",
    "\n",
    "            y_true.extend(y_batch.cpu().numpy())\n",
    "            y_scores.extend(probabilities)\n",
    "\n",
    "        test_auc = roc_auc_score(y_true, y_scores)\n",
    "        print(f'Dataset {dataset_id}, Test AUC with CoTTA: {test_auc:.4f}')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Training Function"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 123,
   "metadata": {},
   "outputs": [],
   "source": [
    "def train_model(model, train_loader, valid_loader, criterion, optimizer, device, epochs, early_stopping_patience=None):\n",
    "    best_loss = float('inf')\n",
    "    epochs_no_improve = 0\n",
    "    early_stop = False\n",
    "\n",
    "    for epoch in range(epochs):\n",
    "        if early_stop:\n",
    "            print(\"Early stopping\")\n",
    "            break\n",
    "\n",
    "        model.train()\n",
    "        total_loss = 0\n",
    "        for x_batch, y_batch in train_loader:\n",
    "            x_batch = x_batch.to(device)\n",
    "            y_batch = y_batch.to(device)\n",
    "            optimizer.zero_grad()\n",
    "            logits = model(x_batch).squeeze()\n",
    "            loss = criterion(logits, y_batch)\n",
    "            loss.backward()\n",
    "            optimizer.step()\n",
    "            total_loss += loss.item()\n",
    "        avg_loss = total_loss / len(train_loader)\n",
    "\n",
    "        # Validation\n",
    "        model.eval()\n",
    "        val_total_loss = 0\n",
    "        y_true = []\n",
    "        y_scores = []\n",
    "        with torch.no_grad():\n",
    "            for x_val, y_val in valid_loader:\n",
    "                x_val = x_val.to(device)\n",
    "                y_val = y_val.to(device)\n",
    "                logits = model(x_val).squeeze()\n",
    "                loss = criterion(logits, y_val)\n",
    "                val_total_loss += loss.item()\n",
    "                y_pred = torch.sigmoid(logits)\n",
    "                y_true.extend(y_val.cpu().numpy())\n",
    "                y_scores.extend(y_pred.cpu().numpy())\n",
    "        val_avg_loss = val_total_loss / len(valid_loader)\n",
    "        val_auc = roc_auc_score(y_true, y_scores)\n",
    "        print(f'Epoch {epoch+1}/{epochs}, Loss: {avg_loss:.4f}, Val Loss: {val_avg_loss:.4f}, Validation AUC: {val_auc:.4f}')\n",
    "\n",
    "        # Early stopping not used since epochs=1\n",
    "        if early_stopping_patience is not None:\n",
    "            if val_avg_loss < best_loss:\n",
    "                best_loss = val_avg_loss\n",
    "                epochs_no_improve = 0\n",
    "                # Save best model\n",
    "                torch.save(model.state_dict(), 'best_model.pth')\n",
    "            else:\n",
    "                epochs_no_improve += 1\n",
    "                if epochs_no_improve >= early_stopping_patience:\n",
    "                    print(f\"Early stopping after {epoch+1} epochs\")\n",
    "                    early_stop = True\n",
    "\n",
    "    if early_stopping_patience is not None:\n",
    "        print(f'Best Validation Loss: {best_loss:.4f}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 124,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Training on dataset 1458\n",
      "Epoch 1/1, Loss: 0.2963, Val Loss: 0.1011, Validation AUC: 0.5419\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/vladplyusnin/.local/lib/python3.10/site-packages/torch/_compile.py:24: UserWarning: optimizer contains a parameter group with duplicate parameters; in future, this will cause an error; see github.com/pytorch/pytorch/issues/40967 for more information\n",
      "  return torch._dynamo.disable(fn, recursive)(*args, **kwargs)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Dataset 1458, Test AUC with CoTTA: 0.5300\n",
      "\n",
      "Training on dataset 2259\n",
      "Epoch 1/1, Loss: 1.0594, Val Loss: 0.5556, Validation AUC: 0.5793\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/vladplyusnin/.local/lib/python3.10/site-packages/torch/_compile.py:24: UserWarning: optimizer contains a parameter group with duplicate parameters; in future, this will cause an error; see github.com/pytorch/pytorch/issues/40967 for more information\n",
      "  return torch._dynamo.disable(fn, recursive)(*args, **kwargs)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Dataset 2259, Test AUC with CoTTA: 0.5103\n",
      "\n",
      "Training on dataset 2261\n",
      "Epoch 1/1, Loss: 0.2766, Val Loss: 0.1582, Validation AUC: 0.5979\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/vladplyusnin/.local/lib/python3.10/site-packages/torch/_compile.py:24: UserWarning: optimizer contains a parameter group with duplicate parameters; in future, this will cause an error; see github.com/pytorch/pytorch/issues/40967 for more information\n",
      "  return torch._dynamo.disable(fn, recursive)(*args, **kwargs)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Dataset 2261, Test AUC with CoTTA: 0.5001\n",
      "\n",
      "Training on dataset 2821\n",
      "Epoch 1/1, Loss: 0.0608, Val Loss: 0.0395, Validation AUC: 0.5471\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/vladplyusnin/.local/lib/python3.10/site-packages/torch/_compile.py:24: UserWarning: optimizer contains a parameter group with duplicate parameters; in future, this will cause an error; see github.com/pytorch/pytorch/issues/40967 for more information\n",
      "  return torch._dynamo.disable(fn, recursive)(*args, **kwargs)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Dataset 2821, Test AUC with CoTTA: 0.5345\n",
      "\n",
      "Training on dataset 2997\n",
      "Epoch 1/1, Loss: 0.0463, Val Loss: 0.0447, Validation AUC: 0.4665\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/vladplyusnin/.local/lib/python3.10/site-packages/torch/_compile.py:24: UserWarning: optimizer contains a parameter group with duplicate parameters; in future, this will cause an error; see github.com/pytorch/pytorch/issues/40967 for more information\n",
      "  return torch._dynamo.disable(fn, recursive)(*args, **kwargs)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Dataset 2997, Test AUC with CoTTA: 0.4907\n",
      "\n",
      "Training on dataset 3358\n",
      "Epoch 1/1, Loss: 0.0450, Val Loss: 0.0245, Validation AUC: 0.5731\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/vladplyusnin/.local/lib/python3.10/site-packages/torch/_compile.py:24: UserWarning: optimizer contains a parameter group with duplicate parameters; in future, this will cause an error; see github.com/pytorch/pytorch/issues/40967 for more information\n",
      "  return torch._dynamo.disable(fn, recursive)(*args, **kwargs)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Dataset 3358, Test AUC with CoTTA: 0.6089\n",
      "\n",
      "Training on dataset 3386\n",
      "Epoch 1/1, Loss: 0.0220, Val Loss: 0.0145, Validation AUC: 0.5573\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/vladplyusnin/.local/lib/python3.10/site-packages/torch/_compile.py:24: UserWarning: optimizer contains a parameter group with duplicate parameters; in future, this will cause an error; see github.com/pytorch/pytorch/issues/40967 for more information\n",
      "  return torch._dynamo.disable(fn, recursive)(*args, **kwargs)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Dataset 3386, Test AUC with CoTTA: 0.5412\n",
      "\n",
      "Training on dataset 3427\n",
      "Epoch 1/1, Loss: 0.0106, Val Loss: 0.0092, Validation AUC: 0.4969\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/vladplyusnin/.local/lib/python3.10/site-packages/torch/_compile.py:24: UserWarning: optimizer contains a parameter group with duplicate parameters; in future, this will cause an error; see github.com/pytorch/pytorch/issues/40967 for more information\n",
      "  return torch._dynamo.disable(fn, recursive)(*args, **kwargs)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Dataset 3427, Test AUC with CoTTA: 0.4577\n",
      "\n",
      "Training on dataset 3476\n",
      "Epoch 1/1, Loss: 0.0071, Val Loss: 0.0056, Validation AUC: 0.5297\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/vladplyusnin/.local/lib/python3.10/site-packages/torch/_compile.py:24: UserWarning: optimizer contains a parameter group with duplicate parameters; in future, this will cause an error; see github.com/pytorch/pytorch/issues/40967 for more information\n",
      "  return torch._dynamo.disable(fn, recursive)(*args, **kwargs)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Dataset 3476, Test AUC with CoTTA: 0.5289\n"
     ]
    }
   ],
   "source": [
    "if __name__ == '__main__':\n",
    "    main()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
