{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import copy\n",
    "from copy import deepcopy\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "import torch.utils.data\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "from sklearn.metrics import roc_auc_score\n",
    "import torch.nn.functional as F"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [],
   "source": [
    "def weight_init(layers):\n",
    "    for layer in layers:\n",
    "        if isinstance(layer, nn.BatchNorm1d):\n",
    "            layer.weight.data.fill_(1)\n",
    "            layer.bias.data.zero_()\n",
    "        elif isinstance(layer, nn.Linear):\n",
    "            n = layer.in_features\n",
    "            y = 1.0 / np.sqrt(n)\n",
    "            layer.weight.data.uniform_(-y, y)\n",
    "            if layer.bias is not None:\n",
    "                layer.bias.data.fill_(0)\n",
    "            # nn.init.kaiming_normal_(layer.weight.data, nonlinearity='relu')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Data Loading and Processing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [],
   "source": [
    "def load_data(data_path):\n",
    "    # Read feature mapping\n",
    "    with open(os.path.join(data_path, 'feat.bid.txt'), 'r') as f:\n",
    "        lines = f.readlines()\n",
    "    feature_nums = int(lines[0].strip())\n",
    "    # Assuming features are already mapped to integers in the data files\n",
    "\n",
    "    # Read train data\n",
    "    train_data = pd.read_csv(os.path.join(data_path, 'train.bid.txt'), header=None)\n",
    "    # Read test data\n",
    "    test_data = pd.read_csv(os.path.join(data_path, 'test.bid.txt'), header=None)\n",
    "\n",
    "    return train_data, test_data, feature_nums"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Dataset Class"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [],
   "source": [
    "class CTRDataset(Dataset):\n",
    "    def __init__(self, data):\n",
    "        # Data columns: click + winning price + hour + time_fraction + timestamp + features\n",
    "        self.labels = data.iloc[:, 0].values.astype(np.float32)  # click labels\n",
    "        self.features = data.iloc[:, 5:].values.astype(np.int64)  # features start from column 5\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.labels)\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        x = self.features[idx]\n",
    "        y = self.labels[idx]\n",
    "        return torch.tensor(x, dtype=torch.long), torch.tensor(y, dtype=torch.float32)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Function to expand embedding layers"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [],
   "source": [
    "def expand_embedding_layer(old_embedding, new_num_embeddings):\n",
    "    old_num_embeddings, embedding_dim = old_embedding.weight.size()\n",
    "    if new_num_embeddings <= old_num_embeddings:\n",
    "        return old_embedding  # No need to expand\n",
    "    # Create new embedding layer\n",
    "    new_embedding = nn.Embedding(new_num_embeddings, embedding_dim)\n",
    "    # Copy weights from old embedding\n",
    "    with torch.no_grad():\n",
    "        new_embedding.weight[:old_num_embeddings] = old_embedding.weight\n",
    "        # Initialize new embeddings\n",
    "        nn.init.xavier_uniform_(new_embedding.weight[old_num_embeddings:])\n",
    "    return new_embedding"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Model Implementations\n",
    "### DeepFM Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [],
   "source": [
    "class DeepFM(nn.Module):\n",
    "    def __init__(self,\n",
    "                 feature_nums,\n",
    "                 field_nums,\n",
    "                 latent_dims,\n",
    "                 output_dim=1):\n",
    "        super(DeepFM, self).__init__()\n",
    "        self.feature_nums = feature_nums\n",
    "        self.field_nums = field_nums\n",
    "        self.latent_dims = latent_dims\n",
    "\n",
    "        # Linear part\n",
    "        self.linear = nn.Embedding(self.feature_nums, output_dim)\n",
    "\n",
    "        # FM embedding\n",
    "        self.feature_embedding = nn.Embedding(self.feature_nums, self.latent_dims)\n",
    "        nn.init.xavier_uniform_(self.feature_embedding.weight.data)\n",
    "\n",
    "        # MLP\n",
    "        deep_input_dims = self.field_nums * self.latent_dims\n",
    "        layers = []\n",
    "\n",
    "        neuron_nums = [300, 300, 300]\n",
    "        for neuron_num in neuron_nums:\n",
    "            layers.append(nn.Linear(deep_input_dims, neuron_num))\n",
    "            # layers.append(nn.BatchNorm1d(neuron_num))  # Uncomment if needed\n",
    "            layers.append(nn.ReLU())\n",
    "            layers.append(nn.Dropout(p=0.2))\n",
    "            deep_input_dims = neuron_num\n",
    "\n",
    "        layers.append(nn.Linear(deep_input_dims, 1))\n",
    "\n",
    "        # Initialize weights\n",
    "        weight_init(layers)\n",
    "\n",
    "        self.mlp = nn.Sequential(*layers)\n",
    "\n",
    "    def expand_embeddings(self, new_feature_nums):\n",
    "        self.linear = expand_embedding_layer(self.linear, new_feature_nums)\n",
    "        self.feature_embedding = expand_embedding_layer(self.feature_embedding, new_feature_nums)\n",
    "        self.feature_nums = new_feature_nums\n",
    "\n",
    "    def forward(self, x):\n",
    "        # Linear Part\n",
    "        linear_out = torch.sum(self.linear(x), dim=1)  # Shape: (batch_size, output_dim)\n",
    "\n",
    "        # FM Part\n",
    "        embedding_x = self.feature_embedding(x)\n",
    "        square_of_sum = torch.sum(embedding_x, dim=1) ** 2\n",
    "        sum_of_square = torch.sum(embedding_x ** 2, dim=1)\n",
    "        ix = 0.5 * torch.sum(square_of_sum - sum_of_square, dim=1, keepdim=True)  # Shape: (batch_size, 1)\n",
    "\n",
    "        # Deep Part\n",
    "        deep_out = self.mlp(embedding_x.view(-1, self.field_nums * self.latent_dims))  # Shape: (batch_size, 1)\n",
    "\n",
    "        # Output\n",
    "        out = linear_out + ix + deep_out  # Shape: (batch_size, 1)\n",
    "\n",
    "        return out  # Return logits"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "FNN Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [],
   "source": [
    "class FNN(nn.Module):\n",
    "    def __init__(self,\n",
    "                 feature_nums,\n",
    "                 field_nums,\n",
    "                 latent_dims):\n",
    "        super(FNN, self).__init__()\n",
    "        self.feature_nums = feature_nums\n",
    "        self.field_nums = field_nums\n",
    "        self.latent_dims = latent_dims\n",
    "\n",
    "        # Embedding layer\n",
    "        self.feature_embedding = nn.Embedding(self.feature_nums, self.latent_dims)\n",
    "        nn.init.xavier_uniform_(self.feature_embedding.weight.data)\n",
    "\n",
    "        # MLP\n",
    "        deep_input_dims = self.field_nums * self.latent_dims\n",
    "        layers = []\n",
    "\n",
    "        neuron_nums = [300, 300, 300]\n",
    "        for neuron_num in neuron_nums:\n",
    "            layers.append(nn.Linear(deep_input_dims, neuron_num))\n",
    "            # layers.append(nn.BatchNorm1d(neuron_num))  # Uncomment if needed\n",
    "            layers.append(nn.ReLU())\n",
    "            layers.append(nn.Dropout(p=0.2))\n",
    "            deep_input_dims = neuron_num\n",
    "\n",
    "        layers.append(nn.Linear(deep_input_dims, 1))\n",
    "\n",
    "        # Initialize weights\n",
    "        weight_init(layers)\n",
    "\n",
    "        self.mlp = nn.Sequential(*layers)\n",
    "\n",
    "    def expand_embeddings(self, new_feature_nums):\n",
    "        self.feature_embedding = expand_embedding_layer(self.feature_embedding, new_feature_nums)\n",
    "        self.feature_nums = new_feature_nums\n",
    "\n",
    "    def forward(self, x):\n",
    "        embedding_x = self.feature_embedding(x)\n",
    "        out = self.mlp(embedding_x.view(-1, self.field_nums * self.latent_dims))\n",
    "\n",
    "        return out  # Return logits\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "DCN Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {},
   "outputs": [],
   "source": [
    "class DCN(nn.Module):\n",
    "    def __init__(self,\n",
    "                 feature_nums,\n",
    "                 field_nums,\n",
    "                 latent_dims,\n",
    "                 output_dim=1):\n",
    "        super(DCN, self).__init__()\n",
    "        self.feature_nums = feature_nums\n",
    "        self.field_nums = field_nums\n",
    "        self.latent_dims = latent_dims\n",
    "\n",
    "        # Embedding layer\n",
    "        self.feature_embedding = nn.Embedding(self.feature_nums, self.latent_dims)\n",
    "        nn.init.xavier_uniform_(self.feature_embedding.weight.data)\n",
    "\n",
    "        # Deep Network\n",
    "        deep_input_dims = self.field_nums * self.latent_dims\n",
    "        deep_net_layers = []\n",
    "        neural_nums = [300, 300, 300]\n",
    "        self.num_neural_layers = 5  # Number of layers in the cross network\n",
    "\n",
    "        for neural_num in neural_nums:\n",
    "            deep_net_layers.append(nn.Linear(deep_input_dims, neural_num))\n",
    "            # deep_net_layers.append(nn.BatchNorm1d(neural_num))  # Uncomment if needed\n",
    "            deep_net_layers.append(nn.ReLU())\n",
    "            deep_net_layers.append(nn.Dropout(p=0.2))\n",
    "            deep_input_dims = neural_num\n",
    "\n",
    "        # Initialize weights\n",
    "        weight_init(deep_net_layers)\n",
    "\n",
    "        self.DN = nn.Sequential(*deep_net_layers)\n",
    "\n",
    "        # Cross Network\n",
    "        cross_input_dims = self.field_nums * self.latent_dims\n",
    "        self.cross_net_w = nn.ModuleList([\n",
    "            nn.Linear(cross_input_dims, cross_input_dims) for _ in range(self.num_neural_layers)\n",
    "        ])\n",
    "\n",
    "        # Initialize weights for cross network\n",
    "        weight_init(self.cross_net_w)\n",
    "\n",
    "        self.cross_net_b = nn.ParameterList([\n",
    "            nn.Parameter(torch.zeros(cross_input_dims)) for _ in range(self.num_neural_layers)\n",
    "        ])\n",
    "\n",
    "        # Final Linear Layer\n",
    "        self.linear = nn.Linear(deep_input_dims + cross_input_dims, output_dim)\n",
    "        # nn.init.xavier_normal_(self.linear.weight)\n",
    "\n",
    "    def expand_embeddings(self, new_feature_nums):\n",
    "        self.feature_embedding = expand_embedding_layer(self.feature_embedding, new_feature_nums)\n",
    "        self.feature_nums = new_feature_nums\n",
    "        # No need to adjust cross network dimensions since field_nums and latent_dims are unchanged\n",
    "\n",
    "    def forward(self, x):\n",
    "        embedding_x = self.feature_embedding(x).view(-1, self.field_nums * self.latent_dims)\n",
    "\n",
    "        # Cross Network\n",
    "        cn_x0 = embedding_x\n",
    "        cn_x = embedding_x\n",
    "        for i in range(self.num_neural_layers):\n",
    "            cn_x_w = self.cross_net_w[i](cn_x)\n",
    "            cn_x = cn_x0 * cn_x_w + self.cross_net_b[i] + cn_x\n",
    "\n",
    "        # Deep Network\n",
    "        dn_x = self.DN(embedding_x)\n",
    "\n",
    "        # Concatenate\n",
    "        x_stack = torch.cat([cn_x, dn_x], dim=1)\n",
    "\n",
    "        # Final output\n",
    "        out = self.linear(x_stack)\n",
    "\n",
    "        return out  # Return logits"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "AFM Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {},
   "outputs": [],
   "source": [
    "class AFM(nn.Module):\n",
    "    def __init__(self,\n",
    "                 feature_nums,\n",
    "                 field_nums,\n",
    "                 latent_dims,\n",
    "                 output_dim=1):\n",
    "        super(AFM, self).__init__()\n",
    "        self.feature_nums = feature_nums\n",
    "        self.field_nums = field_nums\n",
    "        self.latent_dims = latent_dims\n",
    "\n",
    "        # Embedding layer\n",
    "        self.feature_embedding = nn.Embedding(self.feature_nums, self.latent_dims)\n",
    "        nn.init.xavier_uniform_(self.feature_embedding.weight.data)\n",
    "\n",
    "        # Prepare index pairs for interactions\n",
    "        self.row, self.col = [], []\n",
    "        for i in range(self.field_nums - 1):\n",
    "            for j in range(i + 1, self.field_nums):\n",
    "                self.row.append(i)\n",
    "                self.col.append(j)\n",
    "\n",
    "        attention_factor = self.latent_dims\n",
    "\n",
    "        # Attention network\n",
    "        self.attention_net = nn.Linear(self.latent_dims, attention_factor)\n",
    "        n = self.attention_net.in_features\n",
    "        y = 1.0 / np.sqrt(n)\n",
    "        self.attention_net.weight.data.uniform_(-y, y)\n",
    "        self.attention_net.bias.data.fill_(0)\n",
    "\n",
    "        self.attention_softmax = nn.Linear(attention_factor, 1)\n",
    "\n",
    "        # Output layers\n",
    "        self.fc = nn.Linear(self.latent_dims, output_dim)\n",
    "\n",
    "        # Linear part\n",
    "        self.linear = nn.Embedding(self.feature_nums, output_dim)\n",
    "        self.bias = nn.Parameter(torch.zeros((output_dim,)))\n",
    "\n",
    "    def expand_embeddings(self, new_feature_nums):\n",
    "        self.feature_embedding = expand_embedding_layer(self.feature_embedding, new_feature_nums)\n",
    "        self.linear = expand_embedding_layer(self.linear, new_feature_nums)\n",
    "        self.feature_nums = new_feature_nums\n",
    "\n",
    "    def forward(self, x):\n",
    "        embedding_x = self.feature_embedding(x)\n",
    "\n",
    "        # Pairwise interactions\n",
    "        row_emb = embedding_x[:, self.row]  # Shape: (batch_size, num_pairs, latent_dims)\n",
    "        col_emb = embedding_x[:, self.col]  # Shape: (batch_size, num_pairs, latent_dims)\n",
    "        inner_product = row_emb * col_emb  # Element-wise multiplication\n",
    "\n",
    "        # Attention mechanism\n",
    "        attn_scores = F.relu(self.attention_net(inner_product))  # Shape: (batch_size, num_pairs, attention_factor)\n",
    "        attn_scores = F.softmax(self.attention_softmax(attn_scores), dim=1)  # Shape: (batch_size, num_pairs, 1)\n",
    "        attn_scores = F.dropout(attn_scores, p=0.2)\n",
    "\n",
    "        # Weighted sum of interactions\n",
    "        attn_output = torch.sum(attn_scores * inner_product, dim=1)  # Shape: (batch_size, latent_dims)\n",
    "        attn_output = F.dropout(attn_output, p=0.2)\n",
    "\n",
    "        # Output\n",
    "        linear_part = self.bias + torch.sum(self.linear(x), dim=1)  # Shape: (batch_size, output_dim)\n",
    "        out = linear_part + self.fc(attn_output)  # Shape: (batch_size, output_dim)\n",
    "\n",
    "        return out  # Return logits"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Training and Testing\n",
    "### Main Function"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {},
   "outputs": [],
   "source": [
    "def main():\n",
    "    dataset_ids = ['1458', '2259', '2261', '2821', '2997', '3358', '3386', '3427', '3476']\n",
    "    device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
    "    batch_size = 1024\n",
    "\n",
    "    # Model parameters\n",
    "    model_name = 'AFM'  # Change as needed\n",
    "    latent_dims = 10\n",
    "    dropout = 0.2\n",
    "    num_layers = 5  # For DCN\n",
    "    attn_size = 32  # For AFM\n",
    "    epochs_per_dataset = 1  # Train for 1 epoch per dataset\n",
    "\n",
    "    # Paths\n",
    "    base_data_path = '/home/vladplyusnin/tftest/Deep-Learning-COPSCI764/Project/data/ipinyou/'  # Adjust this path as needed\n",
    "\n",
    "    # Initialize the model (we need to know feature_nums and field_nums)\n",
    "    # Initialize variables\n",
    "    max_feature_nums = 0\n",
    "    field_nums = None  # Will be set after loading the first dataset\n",
    "\n",
    "    # Initialize model as None\n",
    "    model = None\n",
    "\n",
    "    # Loss and optimizer (initialized later)\n",
    "    criterion = nn.BCEWithLogitsLoss()\n",
    "    optimizer = None\n",
    "\n",
    "    for dataset_id in dataset_ids:\n",
    "        data_path = os.path.join(base_data_path, dataset_id)\n",
    "\n",
    "        # Load data\n",
    "        train_data, test_data, feature_nums_dataset = load_data(data_path)\n",
    "        field_nums_dataset = train_data.shape[1] - 5\n",
    "\n",
    "        # Update max_feature_nums if needed\n",
    "        if feature_nums_dataset > max_feature_nums:\n",
    "            max_feature_nums = feature_nums_dataset\n",
    "\n",
    "        # Initialize field_nums if not set\n",
    "        if field_nums is None:\n",
    "            field_nums = field_nums_dataset\n",
    "\n",
    "        # Check that field_nums are the same\n",
    "        if field_nums != field_nums_dataset:\n",
    "            print(f\"Dataset {dataset_id} has different field_nums.\")\n",
    "            # Handle this case as needed\n",
    "            continue  # Skip this dataset for now\n",
    "\n",
    "        # Expand model's embeddings if feature_nums increased\n",
    "        if model is None:\n",
    "            # Initialize the model\n",
    "            if model_name == 'DeepFM':\n",
    "                model = DeepFM(max_feature_nums, field_nums, latent_dims).to(device)\n",
    "            elif model_name == 'FNN':\n",
    "                model = FNN(max_feature_nums, field_nums, latent_dims).to(device)\n",
    "            elif model_name == 'DCN':\n",
    "                model = DCN(max_feature_nums, field_nums, latent_dims).to(device)\n",
    "            elif model_name == 'AFM':\n",
    "                model = AFM(max_feature_nums, field_nums, latent_dims).to(device)\n",
    "            else:\n",
    "                raise ValueError('Unknown model name')\n",
    "            # Initialize optimizer\n",
    "            optimizer = optim.Adam(model.parameters(), lr=1e-4, weight_decay=1e-5)\n",
    "        else:\n",
    "            # Expand embeddings if needed\n",
    "            if feature_nums_dataset > model.feature_nums:\n",
    "                model.expand_embeddings(feature_nums_dataset)\n",
    "                # Update optimizer with new parameters\n",
    "                optimizer = optim.Adam(model.parameters(), lr=1e-4, weight_decay=1e-5)\n",
    "\n",
    "        # Split train data into training and validation sets\n",
    "        from sklearn.model_selection import train_test_split\n",
    "        train_df, val_df = train_test_split(train_data, test_size=0.2, random_state=42)\n",
    "\n",
    "        # Create datasets and loaders\n",
    "        train_dataset = CTRDataset(train_df)\n",
    "        val_dataset = CTRDataset(val_df)\n",
    "        test_dataset = CTRDataset(test_data)\n",
    "\n",
    "        train_loader = DataLoader(train_dataset, batch_size=batch_size, shuffle=True, num_workers=4)\n",
    "        val_loader = DataLoader(val_dataset, batch_size=batch_size, num_workers=4)\n",
    "        test_loader = DataLoader(test_dataset, batch_size=batch_size, num_workers=4)\n",
    "\n",
    "        print(f\"\\nTraining on dataset {dataset_id}\")\n",
    "        # Train the model for 1 epoch on the current dataset\n",
    "        train_model(model, train_loader, val_loader, criterion, optimizer, device, epochs=1, early_stopping_patience=None)\n",
    "\n",
    "        # Testing the model on the test set\n",
    "        model.eval()\n",
    "        y_true = []\n",
    "        y_scores = []\n",
    "\n",
    "        with torch.no_grad():\n",
    "            for x_batch, y_batch in test_loader:\n",
    "                x_batch = x_batch.to(device)\n",
    "                y_batch = y_batch.to(device)\n",
    "\n",
    "                outputs = model(x_batch)  # Outputs are logits\n",
    "                probabilities = torch.sigmoid(outputs).detach().cpu().numpy().flatten()\n",
    "\n",
    "                y_true.extend(y_batch.cpu().numpy())\n",
    "                y_scores.extend(probabilities)\n",
    "\n",
    "        test_auc = roc_auc_score(y_true, y_scores)\n",
    "        print(f'Dataset {dataset_id}, Test AUC: {test_auc:.4f}')\n",
    "\n",
    "    # Optionally, save the final model\n",
    "    torch.save(model.state_dict(), f'{model_name}_final_model.pth')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Training Function"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {},
   "outputs": [],
   "source": [
    "def train_model(model, train_loader, valid_loader, criterion, optimizer, device, epochs, early_stopping_patience=None):\n",
    "    best_loss = float('inf')\n",
    "    epochs_no_improve = 0\n",
    "    early_stop = False\n",
    "\n",
    "    for epoch in range(epochs):\n",
    "        if early_stop:\n",
    "            print(\"Early stopping\")\n",
    "            break\n",
    "\n",
    "        model.train()\n",
    "        total_loss = 0\n",
    "        for x_batch, y_batch in train_loader:\n",
    "            x_batch = x_batch.to(device)\n",
    "            y_batch = y_batch.to(device)\n",
    "            optimizer.zero_grad()\n",
    "            logits = model(x_batch).squeeze()\n",
    "            loss = criterion(logits, y_batch)\n",
    "            loss.backward()\n",
    "            optimizer.step()\n",
    "            total_loss += loss.item()\n",
    "        avg_loss = total_loss / len(train_loader)\n",
    "\n",
    "        # Validation\n",
    "        model.eval()\n",
    "        val_total_loss = 0\n",
    "        y_true = []\n",
    "        y_scores = []\n",
    "        with torch.no_grad():\n",
    "            for x_val, y_val in valid_loader:\n",
    "                x_val = x_val.to(device)\n",
    "                y_val = y_val.to(device)\n",
    "                logits = model(x_val).squeeze()\n",
    "                loss = criterion(logits, y_val)\n",
    "                val_total_loss += loss.item()\n",
    "                y_pred = torch.sigmoid(logits)\n",
    "                y_true.extend(y_val.cpu().numpy())\n",
    "                y_scores.extend(y_pred.cpu().numpy())\n",
    "        val_avg_loss = val_total_loss / len(valid_loader)\n",
    "        val_auc = roc_auc_score(y_true, y_scores)\n",
    "        print(f'Epoch {epoch+1}/{epochs}, Loss: {avg_loss:.4f}, Val Loss: {val_avg_loss:.4f}, Validation AUC: {val_auc:.4f}')\n",
    "\n",
    "        # Early stopping not used since epochs=1\n",
    "        if early_stopping_patience is not None:\n",
    "            if val_avg_loss < best_loss:\n",
    "                best_loss = val_avg_loss\n",
    "                epochs_no_improve = 0\n",
    "                # Save best model\n",
    "                torch.save(model.state_dict(), 'best_model.pth')\n",
    "            else:\n",
    "                epochs_no_improve += 1\n",
    "                if epochs_no_improve >= early_stopping_patience:\n",
    "                    print(f\"Early stopping after {epoch+1} epochs\")\n",
    "                    early_stop = True\n",
    "\n",
    "    if early_stopping_patience is not None:\n",
    "        print(f'Best Validation Loss: {best_loss:.4f}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Training on dataset 1458\n",
      "Epoch 1/1, Loss: 0.1490, Val Loss: 0.0501, Validation AUC: 0.4532\n",
      "Dataset 1458, Test AUC: 0.4495\n",
      "\n",
      "Training on dataset 2259\n",
      "Epoch 1/1, Loss: 0.0259, Val Loss: 0.0184, Validation AUC: 0.4865\n",
      "Dataset 2259, Test AUC: 0.4558\n",
      "\n",
      "Training on dataset 2261\n",
      "Epoch 1/1, Loss: 0.0835, Val Loss: 0.0346, Validation AUC: 0.5741\n",
      "Dataset 2261, Test AUC: 0.4965\n",
      "\n",
      "Training on dataset 2821\n",
      "Epoch 1/1, Loss: 0.0115, Val Loss: 0.0101, Validation AUC: 0.4851\n",
      "Dataset 2821, Test AUC: 0.5028\n",
      "\n",
      "Training on dataset 2997\n",
      "Epoch 1/1, Loss: 0.0535, Val Loss: 0.0518, Validation AUC: 0.4296\n",
      "Dataset 2997, Test AUC: 0.4941\n",
      "\n",
      "Training on dataset 3358\n",
      "Epoch 1/1, Loss: 0.0086, Val Loss: 0.0083, Validation AUC: 0.5906\n",
      "Dataset 3358, Test AUC: 0.6763\n",
      "\n",
      "Training on dataset 3386\n",
      "Epoch 1/1, Loss: 0.0174, Val Loss: 0.0098, Validation AUC: 0.4997\n",
      "Dataset 3386, Test AUC: 0.4977\n",
      "\n",
      "Training on dataset 3427\n",
      "Epoch 1/1, Loss: 0.0106, Val Loss: 0.0083, Validation AUC: 0.5429\n",
      "Dataset 3427, Test AUC: 0.5639\n",
      "\n",
      "Training on dataset 3476\n",
      "Epoch 1/1, Loss: 0.0067, Val Loss: 0.0053, Validation AUC: 0.5308\n",
      "Dataset 3476, Test AUC: 0.4998\n"
     ]
    }
   ],
   "source": [
    "if __name__ == '__main__':\n",
    "    main()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
