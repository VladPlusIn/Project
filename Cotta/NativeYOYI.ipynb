{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import pickle\n",
    "import copy\n",
    "from copy import deepcopy\n",
    "import numpy as np\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "import torch.utils.data\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "from sklearn.metrics import roc_auc_score\n",
    "import torch.nn.functional as F"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 1. Utility Functions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "metadata": {},
   "outputs": [],
   "source": [
    "def weight_init(layers):\n",
    "    for layer in layers:\n",
    "        if isinstance(layer, nn.BatchNorm1d):\n",
    "            layer.weight.data.fill_(1)\n",
    "            layer.bias.data.zero_()\n",
    "        elif isinstance(layer, nn.Linear):\n",
    "            n = layer.in_features\n",
    "            y = 1.0 / np.sqrt(n)\n",
    "            layer.weight.data.uniform_(-y, y)\n",
    "            layer.bias.data.fill_(0)\n",
    "            # nn.init.kaiming_normal_(layer.weight.data, nonlinearity='relu')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 2. Data Handling Functions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "metadata": {},
   "outputs": [],
   "source": [
    "def parse_line(line):\n",
    "    parts = line.strip().split('\\t')\n",
    "    label = int(parts[0])\n",
    "    # Skip parts[1] (the second element)\n",
    "    feature_ids = []\n",
    "    for item in parts[2:]:\n",
    "        feature_id = int(item.split(':')[0])\n",
    "        feature_ids.append(feature_id)\n",
    "    return label, feature_ids\n",
    "\n",
    "def build_feature_mapping(file_paths):\n",
    "    feature_set = set()\n",
    "    for file_path in file_paths:\n",
    "        with open(file_path, 'r') as f:\n",
    "            for line in f:\n",
    "                _, feature_ids = parse_line(line)\n",
    "                feature_set.update(feature_ids)\n",
    "    feature_list = sorted(feature_set)\n",
    "    feature_id_map = {feature_id: idx + 1 for idx, feature_id in enumerate(feature_list)}  # Start from 1\n",
    "    return feature_id_map, len(feature_id_map) + 1  # +1 to account for padding index 0\n",
    "\n",
    "def compute_max_length(file_paths):\n",
    "    max_length = 0\n",
    "    for file_path in file_paths:\n",
    "        with open(file_path, 'r') as f:\n",
    "            for line in f:\n",
    "                _, feature_ids = parse_line(line)\n",
    "                max_length = max(max_length, len(feature_ids))\n",
    "    return max_length"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 3. Custom IterableDataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "metadata": {},
   "outputs": [],
   "source": [
    "class YOYIDataset(torch.utils.data.IterableDataset):\n",
    "    def __init__(self, file_path, max_length, feature_id_map):\n",
    "        super(YOYIDataset).__init__()\n",
    "        self.file_path = file_path\n",
    "        self.max_length = max_length\n",
    "        self.feature_id_map = feature_id_map\n",
    "\n",
    "    def __iter__(self):\n",
    "        worker_info = torch.utils.data.get_worker_info()\n",
    "        if worker_info is None:\n",
    "            # Single-process data loading\n",
    "            yield from self._data_generator(self.file_path)\n",
    "        else:\n",
    "            # Multi-process data loading\n",
    "            total_workers = worker_info.num_workers\n",
    "            worker_id = worker_info.id\n",
    "            # Split workload among workers\n",
    "            yield from self._data_generator(self.file_path, worker_id, total_workers)\n",
    "\n",
    "    def _data_generator(self, file_path, worker_id=0, total_workers=1):\n",
    "        with open(file_path, 'r') as f:\n",
    "            for idx, line in enumerate(f):\n",
    "                if idx % total_workers != worker_id:\n",
    "                    continue\n",
    "                label, feature_ids = parse_line(line)\n",
    "                # Map feature IDs\n",
    "                mapped_feature_ids = [self.feature_id_map.get(fid, 0) for fid in feature_ids]\n",
    "                # Pad feature_ids to max_length\n",
    "                padded_feature_ids = mapped_feature_ids + [0] * (self.max_length - len(mapped_feature_ids))\n",
    "                x = torch.tensor(padded_feature_ids, dtype=torch.long)\n",
    "                y = torch.tensor(label, dtype=torch.float32)\n",
    "                yield x, y\n",
    "\n",
    "def create_data_loader(file_path, max_length, batch_size, num_workers, feature_id_map):\n",
    "    dataset = YOYIDataset(file_path, max_length, feature_id_map)\n",
    "    data_loader = DataLoader(dataset, batch_size=batch_size, num_workers=num_workers)\n",
    "    return data_loader"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 4. Model Definitions\n",
    "### DeepFM Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "metadata": {},
   "outputs": [],
   "source": [
    "class DeepFM(nn.Module):\n",
    "    def __init__(self, feature_nums, field_nums, latent_dims, output_dim=1):\n",
    "        super(DeepFM, self).__init__()\n",
    "        self.field_nums = field_nums\n",
    "        self.latent_dims = latent_dims\n",
    "        self.feature_nums = feature_nums\n",
    "\n",
    "        # Linear part\n",
    "        self.linear = nn.Embedding(self.feature_nums, output_dim, padding_idx=0)\n",
    "\n",
    "        # FM embedding\n",
    "        self.feature_embedding = nn.Embedding(self.feature_nums, self.latent_dims, padding_idx=0)\n",
    "        nn.init.xavier_uniform_(self.feature_embedding.weight.data)\n",
    "\n",
    "        # MLP\n",
    "        deep_input_dims = self.field_nums * self.latent_dims\n",
    "        layers = []\n",
    "\n",
    "        neuron_nums = [300, 300, 300]\n",
    "        for neuron_num in neuron_nums:\n",
    "            layers.append(nn.Linear(deep_input_dims, neuron_num))\n",
    "            # layers.append(nn.BatchNorm1d(neuron_num))  # Uncomment if needed\n",
    "            layers.append(nn.ReLU())\n",
    "            layers.append(nn.Dropout(p=0.2))\n",
    "            deep_input_dims = neuron_num\n",
    "\n",
    "        layers.append(nn.Linear(deep_input_dims, 1))\n",
    "\n",
    "        # Initialize weights\n",
    "        weight_init(layers)\n",
    "\n",
    "        self.mlp = nn.Sequential(*layers)\n",
    "\n",
    "    def forward(self, x):\n",
    "        # Linear Part\n",
    "        linear_out = torch.sum(self.linear(x), dim=1)  # Shape: (batch_size, output_dim)\n",
    "\n",
    "        # FM Part\n",
    "        embedding_x = self.feature_embedding(x)\n",
    "        square_of_sum = torch.sum(embedding_x, dim=1) ** 2\n",
    "        sum_of_square = torch.sum(embedding_x ** 2, dim=1)\n",
    "        ix = 0.5 * torch.sum(square_of_sum - sum_of_square, dim=1, keepdim=True)  # Shape: (batch_size, 1)\n",
    "\n",
    "        # Deep Part\n",
    "        deep_out = self.mlp(embedding_x.view(-1, self.field_nums * self.latent_dims))  # Shape: (batch_size, 1)\n",
    "\n",
    "        # Output\n",
    "        out = linear_out + ix + deep_out  # Shape: (batch_size, 1)\n",
    "\n",
    "        return out  # Return logits"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "FNN Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "metadata": {},
   "outputs": [],
   "source": [
    "class FNN(nn.Module):\n",
    "    def __init__(self, feature_nums, field_nums, latent_dims):\n",
    "        super(FNN, self).__init__()\n",
    "        self.field_nums = field_nums\n",
    "        self.latent_dims = latent_dims\n",
    "\n",
    "        # Embedding layer\n",
    "        self.feature_embedding = nn.Embedding(feature_nums, latent_dims, padding_idx=0)\n",
    "        nn.init.xavier_uniform_(self.feature_embedding.weight.data)\n",
    "\n",
    "        # MLP\n",
    "        deep_input_dims = field_nums * latent_dims\n",
    "        layers = []\n",
    "\n",
    "        neuron_nums = [300, 300, 300]\n",
    "        for neuron_num in neuron_nums:\n",
    "            layers.append(nn.Linear(deep_input_dims, neuron_num))\n",
    "            # layers.append(nn.BatchNorm1d(neuron_num))  # Uncomment if needed\n",
    "            layers.append(nn.ReLU())\n",
    "            layers.append(nn.Dropout(0.2))\n",
    "            deep_input_dims = neuron_num\n",
    "\n",
    "        layers.append(nn.Linear(deep_input_dims, 1))\n",
    "\n",
    "        # Initialize weights\n",
    "        weight_init(layers)\n",
    "\n",
    "        self.mlp = nn.Sequential(*layers)\n",
    "\n",
    "    def forward(self, x):\n",
    "        embedding_x = self.feature_embedding(x)\n",
    "        out = self.mlp(embedding_x.view(-1, self.field_nums * self.latent_dims))\n",
    "        return out  # Return logits"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "DCN Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "metadata": {},
   "outputs": [],
   "source": [
    "class DCN(nn.Module):\n",
    "    def __init__(self, feature_nums, field_nums, latent_dims, output_dim=1):\n",
    "        super(DCN, self).__init__()\n",
    "        self.field_nums = field_nums\n",
    "        self.latent_dims = latent_dims\n",
    "\n",
    "        # Embedding layer\n",
    "        self.feature_embedding = nn.Embedding(feature_nums, latent_dims, padding_idx=0)\n",
    "        nn.init.xavier_uniform_(self.feature_embedding.weight.data)\n",
    "\n",
    "        # Deep Network\n",
    "        deep_input_dims = field_nums * latent_dims\n",
    "        deep_net_layers = []\n",
    "        neural_nums = [300, 300, 300]\n",
    "        self.num_neural_layers = 5  # Number of layers in the cross network\n",
    "\n",
    "        for neural_num in neural_nums:\n",
    "            deep_net_layers.append(nn.Linear(deep_input_dims, neural_num))\n",
    "            # deep_net_layers.append(nn.BatchNorm1d(neural_num))  # Uncomment if needed\n",
    "            deep_net_layers.append(nn.ReLU())\n",
    "            deep_net_layers.append(nn.Dropout(0.2))\n",
    "            deep_input_dims = neural_num\n",
    "\n",
    "        # Initialize weights\n",
    "        weight_init(deep_net_layers)\n",
    "\n",
    "        self.DN = nn.Sequential(*deep_net_layers)\n",
    "\n",
    "        # Cross Network\n",
    "        cross_input_dims = field_nums * latent_dims\n",
    "        self.cross_net_w = nn.ModuleList([\n",
    "            nn.Linear(cross_input_dims, cross_input_dims) for _ in range(self.num_neural_layers)\n",
    "        ])\n",
    "\n",
    "        # Initialize weights for cross network\n",
    "        weight_init(self.cross_net_w)\n",
    "\n",
    "        self.cross_net_b = nn.ParameterList([\n",
    "            nn.Parameter(torch.zeros(cross_input_dims)) for _ in range(self.num_neural_layers)\n",
    "        ])\n",
    "\n",
    "        # Final Linear Layer\n",
    "        self.linear = nn.Linear(deep_input_dims + cross_input_dims, output_dim)\n",
    "        # nn.init.xavier_normal_(self.linear.weight)\n",
    "\n",
    "    def forward(self, x):\n",
    "        embedding_x = self.feature_embedding(x).view(-1, self.field_nums * self.latent_dims)\n",
    "\n",
    "        # Cross Network\n",
    "        cn_x0 = embedding_x\n",
    "        cn_x = embedding_x\n",
    "        for i in range(self.num_neural_layers):\n",
    "            cn_x_w = self.cross_net_w[i](cn_x)\n",
    "            cn_x = cn_x0 * cn_x_w + self.cross_net_b[i] + cn_x\n",
    "\n",
    "        # Deep Network\n",
    "        dn_x = self.DN(embedding_x)\n",
    "\n",
    "        # Concatenate\n",
    "        x_stack = torch.cat([cn_x, dn_x], dim=1)\n",
    "\n",
    "        # Final output\n",
    "        out = self.linear(x_stack)\n",
    "\n",
    "        return out  # Return logits"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "AFM Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "metadata": {},
   "outputs": [],
   "source": [
    "class AFM(nn.Module):\n",
    "    def __init__(self, feature_nums, field_nums, latent_dims, output_dim=1):\n",
    "        super(AFM, self).__init__()\n",
    "        self.field_nums = field_nums\n",
    "        self.latent_dims = latent_dims\n",
    "\n",
    "        # Embedding layer\n",
    "        self.feature_embedding = nn.Embedding(feature_nums, latent_dims, padding_idx=0)\n",
    "        nn.init.xavier_uniform_(self.feature_embedding.weight.data)\n",
    "\n",
    "        # Prepare index pairs for interactions\n",
    "        self.row, self.col = [], []\n",
    "        for i in range(self.field_nums - 1):\n",
    "            for j in range(i + 1, self.field_nums):\n",
    "                self.row.append(i)\n",
    "                self.col.append(j)\n",
    "\n",
    "        attention_factor = self.latent_dims\n",
    "\n",
    "        # Attention network\n",
    "        self.attention_net = nn.Linear(self.latent_dims, attention_factor)\n",
    "        n = self.attention_net.in_features\n",
    "        y = 1.0 / np.sqrt(n)\n",
    "        self.attention_net.weight.data.uniform_(-y, y)\n",
    "        self.attention_net.bias.data.fill_(0)\n",
    "\n",
    "        self.attention_softmax = nn.Linear(attention_factor, 1)\n",
    "\n",
    "        # Output layers\n",
    "        self.fc = nn.Linear(self.latent_dims, output_dim)\n",
    "\n",
    "        # Linear part\n",
    "        self.linear = nn.Embedding(feature_nums, output_dim, padding_idx=0)\n",
    "        self.bias = nn.Parameter(torch.zeros((output_dim,)))\n",
    "\n",
    "    def forward(self, x):\n",
    "        embedding_x = self.feature_embedding(x)\n",
    "\n",
    "        # Pairwise interactions\n",
    "        row_emb = embedding_x[:, self.row]  # Shape: (batch_size, num_pairs, latent_dims)\n",
    "        col_emb = embedding_x[:, self.col]  # Shape: (batch_size, num_pairs, latent_dims)\n",
    "        inner_product = row_emb * col_emb  # Element-wise multiplication\n",
    "\n",
    "        # Attention mechanism\n",
    "        attn_scores = F.relu(self.attention_net(inner_product))  # Shape: (batch_size, num_pairs, attention_factor)\n",
    "        attn_scores = F.softmax(self.attention_softmax(attn_scores), dim=1)  # Shape: (batch_size, num_pairs, 1)\n",
    "        attn_scores = F.dropout(attn_scores, p=0.2)\n",
    "\n",
    "        # Weighted sum of interactions\n",
    "        attn_output = torch.sum(attn_scores * inner_product, dim=1)  # Shape: (batch_size, latent_dims)\n",
    "        attn_output = F.dropout(attn_output, p=0.2)\n",
    "\n",
    "        # Output\n",
    "        linear_part = self.bias + torch.sum(self.linear(x), dim=1)  # Shape: (batch_size, output_dim)\n",
    "        out = linear_part + self.fc(attn_output)  # Shape: (batch_size, output_dim)\n",
    "\n",
    "        return out  # Return logits"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 5. Training Function"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "metadata": {},
   "outputs": [],
   "source": [
    "def train_model(model, train_loader, valid_loader, criterion, optimizer, device, epochs, early_stopping_patience=2):\n",
    "    best_loss = float('inf')\n",
    "    epochs_no_improve = 0\n",
    "    early_stop = False\n",
    "\n",
    "    for epoch in range(epochs):\n",
    "        if early_stop:\n",
    "            print(\"Early stopping\")\n",
    "            break\n",
    "\n",
    "        model.train()\n",
    "        total_loss = 0\n",
    "        num_batches = 0  # Keep track of the number of batches\n",
    "        for x_batch, y_batch in train_loader:\n",
    "            x_batch = x_batch.to(device)\n",
    "            y_batch = y_batch.to(device)\n",
    "            optimizer.zero_grad()\n",
    "            logits = model(x_batch).squeeze()\n",
    "            loss = criterion(logits, y_batch)\n",
    "            loss.backward()\n",
    "            optimizer.step()\n",
    "            total_loss += loss.item()\n",
    "            num_batches += 1\n",
    "        avg_loss = total_loss / num_batches if num_batches > 0 else 0\n",
    "        print(f'Epoch {epoch+1}/{epochs}, Loss: {avg_loss:.4f}')\n",
    "\n",
    "        # Validation\n",
    "        if valid_loader is not None:\n",
    "            model.eval()\n",
    "            val_total_loss = 0\n",
    "            num_val_batches = 0  # Keep track of the number of validation batches\n",
    "            y_true = []\n",
    "            y_scores = []\n",
    "            with torch.no_grad():\n",
    "                for x_val, y_val in valid_loader:\n",
    "                    x_val = x_val.to(device)\n",
    "                    y_val = y_val.to(device)\n",
    "                    logits = model(x_val).squeeze()\n",
    "                    loss = criterion(logits, y_val)\n",
    "                    val_total_loss += loss.item()\n",
    "                    num_val_batches += 1\n",
    "                    y_pred = torch.sigmoid(logits)\n",
    "                    y_true.extend(y_val.cpu().numpy())\n",
    "                    y_scores.extend(y_pred.cpu().numpy())\n",
    "            val_avg_loss = val_total_loss / num_val_batches if num_val_batches > 0 else 0\n",
    "            val_auc = roc_auc_score(y_true, y_scores)\n",
    "            print(f'Val Loss: {val_avg_loss:.4f}, Validation AUC: {val_auc:.4f}')\n",
    "\n",
    "            # Check for early stopping\n",
    "            if val_avg_loss < best_loss:\n",
    "                best_loss = val_avg_loss\n",
    "                epochs_no_improve = 0\n",
    "                # Save best model\n",
    "                torch.save(model.state_dict(), 'base_best_model_AFM.pth')\n",
    "            else:\n",
    "                epochs_no_improve += 1\n",
    "                if epochs_no_improve >= early_stopping_patience:\n",
    "                    print(f\"Early stopping after {epoch+1} epochs\")\n",
    "                    early_stop = True\n",
    "        else:\n",
    "            # Save model every epoch if no validation set\n",
    "            torch.save(model.state_dict(), 'base_best_model_AFM.pth')\n",
    "\n",
    "    if valid_loader is not None:\n",
    "        print(f'Best Validation Loss: {best_loss:.4f}')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 6. Testing Function"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "metadata": {},
   "outputs": [],
   "source": [
    "def test_model(model, test_loader, device):\n",
    "    model.eval()\n",
    "    y_true = []\n",
    "    y_scores = []\n",
    "\n",
    "    with torch.no_grad():\n",
    "        for x_batch, y_batch in test_loader:\n",
    "            x_batch = x_batch.to(device)\n",
    "            y_batch = y_batch.to(device)\n",
    "\n",
    "            logits = model(x_batch).squeeze()\n",
    "            probabilities = torch.sigmoid(logits).cpu().numpy().flatten()\n",
    "\n",
    "            y_true.extend(y_batch.cpu().numpy())\n",
    "            y_scores.extend(probabilities)\n",
    "\n",
    "    test_auc = roc_auc_score(y_true, y_scores)\n",
    "    print(f'Test AUC: {test_auc:.4f}')\n",
    "    return test_auc\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 7. Main Function"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "metadata": {},
   "outputs": [],
   "source": [
    "def main():\n",
    "    data_path = '/home/vladplyusnin/tftest/Deep-Learning-COPSCI764/Project/make-ipinyou-data/cikm2016-yoyi-dataset/'  # Adjust this path as needed\n",
    "\n",
    "    train_file = os.path.join(data_path, 'train_set.txt')\n",
    "    test_file = os.path.join(data_path, 'test_set.txt')\n",
    "\n",
    "    # Paths to save feature mapping and max_length\n",
    "    feature_map_path = os.path.join(data_path, 'feature_id_map.pkl')\n",
    "    max_length_path = os.path.join(data_path, 'max_length.pkl')\n",
    "\n",
    "    # Check if feature mapping and max_length files exist\n",
    "    if os.path.exists(feature_map_path) and os.path.exists(max_length_path):\n",
    "        print(\"Loading feature mapping and max_length from disk...\")\n",
    "        with open(feature_map_path, 'rb') as f:\n",
    "            feature_id_map, feature_nums = pickle.load(f)\n",
    "        with open(max_length_path, 'rb') as f:\n",
    "            max_length = pickle.load(f)\n",
    "    else:\n",
    "        # Build feature mapping and get feature_nums\n",
    "        print(\"Building feature mapping...\")\n",
    "        feature_id_map, feature_nums = build_feature_mapping([train_file, test_file])\n",
    "        print(f\"Total number of features: {feature_nums}\")\n",
    "\n",
    "        # Compute max_length\n",
    "        print(\"Computing maximum feature length...\")\n",
    "        max_length = compute_max_length([train_file, test_file])\n",
    "        print(f\"Maximum feature length: {max_length}\")\n",
    "\n",
    "        # Save feature mapping and max_length to disk\n",
    "        with open(feature_map_path, 'wb') as f:\n",
    "            pickle.dump((feature_id_map, feature_nums), f)\n",
    "        with open(max_length_path, 'wb') as f:\n",
    "            pickle.dump(max_length, f)\n",
    "\n",
    "    field_nums = max_length  # Since we've padded features to max_length\n",
    "\n",
    "    batch_size = 2048\n",
    "    num_workers = 12  # Adjust based on your system\n",
    "\n",
    "    # Create data loaders\n",
    "    print(\"Creating data loaders...\")\n",
    "    train_loader = create_data_loader(train_file, max_length, batch_size, num_workers, feature_id_map)\n",
    "    test_loader = create_data_loader(test_file, max_length, batch_size, num_workers, feature_id_map)\n",
    "\n",
    "    # Since we cannot split the data for validation easily, we'll set val_loader to None\n",
    "    val_loader = None  # Set to None if not using validation\n",
    "\n",
    "    # Model parameters\n",
    "    model_name = 'AFM'  # Change this to 'DeepFM', 'FNN', 'DCN', or 'AFM' as needed\n",
    "    latent_dims = 10\n",
    "    dropout = 0.2\n",
    "    num_layers = 5  # For DCN\n",
    "    attn_size = 32  # For AFM\n",
    "    device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
    "    epochs = 1  # Adjust as needed\n",
    "\n",
    "    # Get model\n",
    "    print(f\"Initializing model: {model_name}\")\n",
    "    if model_name == 'DeepFM':\n",
    "        model = DeepFM(feature_nums, field_nums, latent_dims).to(device)\n",
    "    elif model_name == 'FNN':\n",
    "        model = FNN(feature_nums, field_nums, latent_dims).to(device)\n",
    "    elif model_name == 'DCN':\n",
    "        model = DCN(feature_nums, field_nums, latent_dims).to(device)\n",
    "    elif model_name == 'AFM':\n",
    "        model = AFM(feature_nums, field_nums, latent_dims).to(device)\n",
    "    else:\n",
    "        raise ValueError('Unknown model name')\n",
    "\n",
    "    # Loss and optimizer\n",
    "    criterion = nn.BCEWithLogitsLoss()\n",
    "    optimizer = optim.Adam(model.parameters(), lr=1e-4, weight_decay=1e-5)\n",
    "\n",
    "    # Training\n",
    "    print(\"Starting training...\")\n",
    "    train_model(model, train_loader, val_loader, criterion, optimizer, device, epochs, early_stopping_patience=2)\n",
    "\n",
    "    # Load best model\n",
    "    #model.load_state_dict(torch.load('base_best_model.pth'))\n",
    "\n",
    "    # Testing\n",
    "    print(\"Starting testing...\")\n",
    "    test_auc = test_model(model, test_loader, device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loading feature mapping and max_length from disk...\n",
      "Creating data loaders...\n",
      "Initializing model: AFM\n",
      "Starting training...\n",
      "Epoch 1/1, Loss: 0.0305\n",
      "Starting testing...\n",
      "Test AUC: 0.8409\n"
     ]
    }
   ],
   "source": [
    "if __name__ == '__main__':\n",
    "    main()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
