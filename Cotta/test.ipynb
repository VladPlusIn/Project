{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 409,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import copy\n",
    "from copy import deepcopy\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "import torch.utils.data\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "from sklearn.metrics import roc_auc_score"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Transformation Function"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 410,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_tta_transforms():\n",
    "    # Define a transformation that randomly masks some features\n",
    "    class RandomFeatureMasking:\n",
    "        def __init__(self, p=0.1):\n",
    "            self.p = p\n",
    "\n",
    "        def __call__(self, x):\n",
    "            # x is a tensor of shape (batch_size, num_features)\n",
    "            # Generate a mask with probability p of masking each feature\n",
    "            mask = (torch.rand_like(x.float()) > self.p).long()\n",
    "            x_transformed = x * mask\n",
    "            return x_transformed\n",
    "\n",
    "    return RandomFeatureMasking(p=0.1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "CoTTA Utility Functions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 411,
   "metadata": {},
   "outputs": [],
   "source": [
    "def copy_model_and_optimizer(model, optimizer):\n",
    "    \"\"\"Copy the model and optimizer states for resetting after adaptation.\"\"\"\n",
    "    model_state = deepcopy(model.state_dict())\n",
    "    model_anchor = deepcopy(model)\n",
    "    optimizer_state = deepcopy(optimizer.state_dict())\n",
    "    ema_model = deepcopy(model)\n",
    "    for param in ema_model.parameters():\n",
    "        param.detach_()\n",
    "    return model_state, optimizer_state, ema_model, model_anchor\n",
    "\n",
    "def load_model_and_optimizer(model, optimizer, model_state, optimizer_state):\n",
    "    \"\"\"Restore the model and optimizer states from copies.\"\"\"\n",
    "    model.load_state_dict(model_state, strict=True)\n",
    "    optimizer.load_state_dict(optimizer_state)\n",
    "\n",
    "def configure_model(model):\n",
    "    \"\"\"Configure model for use with CoTTA.\"\"\"\n",
    "    model.train()\n",
    "    model.requires_grad_(False)\n",
    "    for m in model.modules():\n",
    "        if isinstance(m, (nn.Embedding, nn.BatchNorm1d)):\n",
    "            m.requires_grad_(True)\n",
    "            if isinstance(m, nn.BatchNorm1d):\n",
    "                m.track_running_stats = False\n",
    "                m.running_mean = None\n",
    "                m.running_var = None\n",
    "    return model\n",
    "\n",
    "def collect_params(model):\n",
    "    \"\"\"Collect all trainable parameters.\"\"\"\n",
    "    params = []\n",
    "    names = []\n",
    "    for nm, m in model.named_modules():\n",
    "        for np, p in m.named_parameters():\n",
    "            if p.requires_grad:\n",
    "                params.append(p)\n",
    "                names.append(f\"{nm}.{np}\")\n",
    "                print(f\"Parameter to adapt: {nm}.{np}\")\n",
    "    return params, names"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Custom Binary Entropy Function"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 412,
   "metadata": {},
   "outputs": [],
   "source": [
    "def binary_entropy(logits, logits_ema):\n",
    "    \"\"\"Entropy for binary classification.\"\"\"\n",
    "    p = torch.sigmoid(logits)\n",
    "    p_ema = torch.sigmoid(logits_ema)\n",
    "    entropy = -0.5 * (p_ema * torch.log(p + 1e-8) + (1 - p_ema) * torch.log(1 - p + 1e-8))\n",
    "    entropy -= 0.5 * (p * torch.log(p_ema + 1e-8) + (1 - p) * torch.log(1 - p_ema + 1e-8))\n",
    "    return entropy.squeeze()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "CoTTA Class"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 413,
   "metadata": {},
   "outputs": [],
   "source": [
    "class CoTTA(nn.Module):\n",
    "    \"\"\"CoTTA adapts a model by entropy minimization during testing.\"\"\"\n",
    "    def __init__(self, model, optimizer, steps=1, episodic=False):\n",
    "        super().__init__()\n",
    "        self.model = model\n",
    "        self.optimizer = optimizer\n",
    "        self.steps = steps\n",
    "        assert steps > 0, \"CoTTA requires >= 1 step(s) to forward and update\"\n",
    "        self.episodic = episodic\n",
    "\n",
    "        self.model_state, self.optimizer_state, self.model_ema, self.model_anchor = \\\n",
    "            copy_model_and_optimizer(self.model, self.optimizer)\n",
    "        self.transform = get_tta_transforms()\n",
    "\n",
    "    def forward(self, x):\n",
    "        if self.episodic:\n",
    "            self.reset()\n",
    "        for _ in range(self.steps):\n",
    "            outputs = self.forward_and_adapt(x, self.model, self.optimizer)\n",
    "        return outputs\n",
    "\n",
    "    def reset(self):\n",
    "        if self.model_state is None or self.optimizer_state is None:\n",
    "            raise Exception(\"Cannot reset without saved model/optimizer state\")\n",
    "        load_model_and_optimizer(self.model, self.optimizer,\n",
    "                                 self.model_state, self.optimizer_state)\n",
    "        self.model_state, self.optimizer_state, self.model_ema, self.model_anchor = \\\n",
    "            copy_model_and_optimizer(self.model, self.optimizer)\n",
    "\n",
    "    @torch.enable_grad()\n",
    "    def forward_and_adapt(self, x, model, optimizer):\n",
    "        outputs = self.model(x)\n",
    "        self.model_ema.train()\n",
    "        # Teacher Prediction\n",
    "        anchor_prob = torch.sigmoid(self.model_anchor(x)).detach()\n",
    "        standard_ema = self.model_ema(x)\n",
    "        # Augmentation-averaged Prediction\n",
    "        N = 32\n",
    "        outputs_emas = []\n",
    "        to_aug = anchor_prob.mean() < 0.1  # Adjusted for binary classification\n",
    "        if to_aug:\n",
    "            for i in range(N):\n",
    "                outputs_ = self.model_ema(self.transform(x)).detach()\n",
    "                outputs_emas.append(outputs_)\n",
    "            outputs_ema = torch.stack(outputs_emas).mean(0)\n",
    "        else:\n",
    "            outputs_ema = standard_ema\n",
    "        # Student update\n",
    "        loss = binary_entropy(outputs, outputs_ema.detach()).mean()\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "        optimizer.zero_grad()\n",
    "        # Teacher update\n",
    "        self.model_ema = update_ema_variables(self.model_ema, self.model, alpha_teacher=0.999)\n",
    "        # Stochastic restore\n",
    "        for nm, m in self.model.named_modules():\n",
    "            for npp, p in m.named_parameters():\n",
    "                if p.requires_grad:\n",
    "                    mask = (torch.rand(p.shape) < 0.001).float().to(p.device)\n",
    "                    with torch.no_grad():\n",
    "                        key = f\"{nm}.{npp}\" if nm else npp  # Corrected key construction\n",
    "                        p.data = self.model_state[key] * mask + p * (1. - mask)\n",
    "        return outputs_ema"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Data Loading and Processing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 414,
   "metadata": {},
   "outputs": [],
   "source": [
    "def load_data(data_path):\n",
    "    # Read feature mapping\n",
    "    with open(os.path.join(data_path, 'feat.bid.txt'), 'r') as f:\n",
    "        lines = f.readlines()\n",
    "    feature_nums = int(lines[0].strip())\n",
    "    # Assuming features are already mapped to integers in the data files\n",
    "\n",
    "    # Read train data\n",
    "    train_data = pd.read_csv(os.path.join(data_path, 'train.bid.txt'), header=None)\n",
    "    # Read test data\n",
    "    test_data = pd.read_csv(os.path.join(data_path, 'test.bid.txt'), header=None)\n",
    "\n",
    "    return train_data, test_data, feature_nums"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Dataset Class"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 415,
   "metadata": {},
   "outputs": [],
   "source": [
    "class CTRDataset(Dataset):\n",
    "    def __init__(self, data):\n",
    "        # Data columns: click + winning price + hour + time_fraction + timestamp + features\n",
    "        self.labels = data.iloc[:, 0].values.astype(np.float32)  # click labels\n",
    "        self.features = data.iloc[:, 5:].values.astype(np.int64)  # features start from column 5\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.labels)\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        x = self.features[idx]\n",
    "        y = self.labels[idx]\n",
    "        return torch.tensor(x, dtype=torch.long), torch.tensor(y, dtype=torch.float32)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Model Implementations\n",
    "### DeepFM Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 416,
   "metadata": {},
   "outputs": [],
   "source": [
    "class DeepFM(nn.Module):\n",
    "    def __init__(self, feature_nums, field_nums, embed_dim, mlp_dims, dropout):\n",
    "        super(DeepFM, self).__init__()\n",
    "        self.field_nums = field_nums\n",
    "\n",
    "        # Linear part\n",
    "        self.linear = nn.Embedding(feature_nums, 1)\n",
    "\n",
    "        # FM part\n",
    "        self.fm_embedding = nn.Embedding(feature_nums, embed_dim)\n",
    "\n",
    "        # Deep part\n",
    "        self.deep_embedding = nn.Embedding(feature_nums, embed_dim)\n",
    "        deep_input_dim = field_nums * embed_dim\n",
    "        layers = []\n",
    "        for dim in mlp_dims:\n",
    "            layers.extend([\n",
    "                nn.Linear(deep_input_dim, dim),\n",
    "                nn.ReLU(),\n",
    "                nn.Dropout(dropout)\n",
    "            ])\n",
    "            deep_input_dim = dim\n",
    "        self.mlp = nn.Sequential(*layers)\n",
    "        self.fc = nn.Linear(deep_input_dim, 1)\n",
    "\n",
    "    def forward(self, x):\n",
    "        # Linear part\n",
    "        linear_out = self.linear(x).sum(1)  # Shape: (batch_size, 1)\n",
    "\n",
    "        # FM part\n",
    "        fm_emb = self.fm_embedding(x)\n",
    "        sum_square = torch.sum(fm_emb, dim=1) ** 2\n",
    "        square_sum = torch.sum(fm_emb ** 2, dim=1)\n",
    "        fm_out = 0.5 * (sum_square - square_sum).sum(1, keepdim=True)  # Shape: (batch_size, 1)\n",
    "\n",
    "        # Deep part\n",
    "        deep_emb = self.deep_embedding(x).view(-1, self.field_nums * fm_emb.size(2))\n",
    "        deep_out = self.mlp(deep_emb)\n",
    "        deep_out = self.fc(deep_out)  # Shape: (batch_size, 1)\n",
    "\n",
    "        # Output logits\n",
    "        total_out = linear_out + fm_out + deep_out  # Shape: (batch_size, 1)\n",
    "        return total_out  # Return logits00000000000fgh00"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "FNN Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 417,
   "metadata": {},
   "outputs": [],
   "source": [
    "class FNN(nn.Module):\n",
    "    def __init__(self, feature_nums, field_nums, embed_dim, mlp_dims, dropout):\n",
    "        super(FNN, self).__init__()\n",
    "        self.field_nums = field_nums\n",
    "\n",
    "        # Embedding layer\n",
    "        self.embedding = nn.Embedding(feature_nums, embed_dim)\n",
    "\n",
    "        # MLP layers\n",
    "        input_dim = field_nums * embed_dim\n",
    "        layers = []\n",
    "        for dim in mlp_dims:\n",
    "            layers.extend([\n",
    "                nn.Linear(input_dim, dim),\n",
    "                nn.ReLU(),\n",
    "                nn.Dropout(dropout)\n",
    "            ])\n",
    "            input_dim = dim\n",
    "        layers.append(nn.Linear(input_dim, 1))\n",
    "        self.mlp = nn.Sequential(*layers)\n",
    "\n",
    "    def forward(self, x):\n",
    "        x_embed = self.embedding(x).view(-1, self.field_nums * self.embedding.embedding_dim)\n",
    "        logits = self.mlp(x_embed)  # Shape: (batch_size, 1)\n",
    "        return logits  # Return logits"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "DCN Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 418,
   "metadata": {},
   "outputs": [],
   "source": [
    "class CrossNetwork(nn.Module):\n",
    "    def __init__(self, input_dim, num_layers):\n",
    "        super(CrossNetwork, self).__init__()\n",
    "        self.num_layers = num_layers\n",
    "        self.cross_layers = nn.ModuleList(\n",
    "            [nn.Linear(input_dim, input_dim, bias=False) for _ in range(num_layers)]\n",
    "        )\n",
    "        self.cross_bias = nn.ParameterList(\n",
    "            [nn.Parameter(torch.zeros(input_dim)) for _ in range(num_layers)]\n",
    "        )\n",
    "\n",
    "    def forward(self, x0):\n",
    "        x = x0\n",
    "        for i in range(self.num_layers):\n",
    "            xw = self.cross_layers[i](x)\n",
    "            x = x0 * xw + self.cross_bias[i] + x\n",
    "        return x\n",
    "\n",
    "class DCN(nn.Module):\n",
    "    def __init__(self, feature_nums, field_nums, embed_dim, num_layers, mlp_dims, dropout):\n",
    "        super(DCN, self).__init__()\n",
    "        self.field_nums = field_nums\n",
    "\n",
    "        # Embedding layer\n",
    "        self.embedding = nn.Embedding(feature_nums, embed_dim)\n",
    "\n",
    "        # Cross Network\n",
    "        input_dim = field_nums * embed_dim\n",
    "        self.cross_network = CrossNetwork(input_dim, num_layers)\n",
    "\n",
    "        # Deep Network\n",
    "        layers = []\n",
    "        for dim in mlp_dims:\n",
    "            layers.extend([\n",
    "                nn.Linear(input_dim, dim),\n",
    "                nn.ReLU(),\n",
    "                nn.Dropout(dropout)\n",
    "            ])\n",
    "            input_dim = dim\n",
    "        self.deep_network = nn.Sequential(*layers)\n",
    "\n",
    "        # Final output layer\n",
    "        self.fc = nn.Linear(input_dim + field_nums * embed_dim, 1)\n",
    "\n",
    "    def forward(self, x):\n",
    "        x_embed = self.embedding(x).view(-1, self.field_nums * self.embedding.embedding_dim)\n",
    "        x_cross = self.cross_network(x_embed)\n",
    "        x_deep = self.deep_network(x_embed)\n",
    "        x_stack = torch.cat([x_cross, x_deep], dim=1)\n",
    "        logits = self.fc(x_stack)  # Shape: (batch_size, 1)\n",
    "        return logits  # Return logits"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "AFM Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 419,
   "metadata": {},
   "outputs": [],
   "source": [
    "class AFM(nn.Module):\n",
    "    def __init__(self, feature_nums, field_nums, embed_dim, attn_size, dropout):\n",
    "        super(AFM, self).__init__()\n",
    "        self.field_nums = field_nums\n",
    "        self.embed_dim = embed_dim\n",
    "\n",
    "        # Embedding layer\n",
    "        self.embedding = nn.Embedding(feature_nums, embed_dim)\n",
    "\n",
    "        # Attention mechanism\n",
    "        self.attention_fc = nn.Sequential(\n",
    "            nn.Linear(embed_dim, attn_size),\n",
    "            nn.ReLU(),\n",
    "            nn.Linear(attn_size, 1)\n",
    "        )\n",
    "        self.dropout = nn.Dropout(dropout)\n",
    "\n",
    "        # Prediction layer\n",
    "        self.fc = nn.Linear(embed_dim, 1)\n",
    "\n",
    "        # Linear part\n",
    "        self.linear = nn.Embedding(feature_nums, 1)\n",
    "\n",
    "    def forward(self, x):\n",
    "        x_embed = self.embedding(x)\n",
    "        linear_out = self.linear(x).sum(1)\n",
    "\n",
    "        # Pairwise interactions\n",
    "        interactions = []\n",
    "        for i in range(self.field_nums):\n",
    "            for j in range(i + 1, self.field_nums):\n",
    "                interactions.append(x_embed[:, i, :] * x_embed[:, j, :])\n",
    "        interactions = torch.stack(interactions, dim=1)  # Shape: (batch_size, num_pairs, embed_dim)\n",
    "\n",
    "        # Attention mechanism\n",
    "        attn_scores = self.attention_fc(interactions).squeeze(-1)  # Shape: (batch_size, num_pairs)\n",
    "        attn_scores = torch.softmax(attn_scores, dim=1)\n",
    "        attn_output = torch.sum(attn_scores.unsqueeze(-1) * interactions, dim=1)  # Shape: (batch_size, embed_dim)\n",
    "        attn_output = self.dropout(attn_output)\n",
    "\n",
    "        # Prediction without sigmoid\n",
    "        logits = linear_out + self.fc(attn_output)  # Shape: (batch_size, 1)\n",
    "        return logits  # Return logits"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Training and Testing with CoTTA\n",
    "### Main Function"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 420,
   "metadata": {},
   "outputs": [],
   "source": [
    "def main():\n",
    "    data_path = '/home/vladplyusnin/tftest/Deep-Learning-COPSCI764/Project/data/ipinyou/1458/'  # Adjust this path as needed\n",
    "    train_data, test_data, feature_nums = load_data(data_path)\n",
    "    field_nums = train_data.shape[1] - 5  # Subtract non-feature columns\n",
    "\n",
    "    # Split train data into training and validation sets\n",
    "    from sklearn.model_selection import train_test_split\n",
    "    train_df, val_df = train_test_split(train_data, test_size=0.2, random_state=42)\n",
    "\n",
    "    # Create datasets and loaders\n",
    "    train_dataset = CTRDataset(train_df)\n",
    "    val_dataset = CTRDataset(val_df)\n",
    "    test_dataset = CTRDataset(test_data)\n",
    "\n",
    "    batch_size = 1024\n",
    "    train_loader = DataLoader(train_dataset, batch_size=batch_size, shuffle=True, num_workers=4)\n",
    "    val_loader = DataLoader(val_dataset, batch_size=batch_size, num_workers=4)\n",
    "    test_loader = DataLoader(test_dataset, batch_size=batch_size, num_workers=4)\n",
    "\n",
    "    # Model parameters\n",
    "    model_name = 'AFM'  # Change this to 'FNN', 'DCN', or 'AFM' as needed\n",
    "    embed_dim = 10\n",
    "    mlp_dims = [300, 300, 300]\n",
    "    dropout = 0.2\n",
    "    num_layers = 5  # For DCN\n",
    "    attn_size = 32  # For AFM\n",
    "    device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
    "    epochs = 100  # Increased to show early stopping effect\n",
    "\n",
    "    # Get model\n",
    "    if model_name == 'DeepFM':\n",
    "        model = DeepFM(feature_nums, field_nums, embed_dim, mlp_dims, dropout).to(device)\n",
    "    elif model_name == 'FNN':\n",
    "        model = FNN(feature_nums, field_nums, embed_dim, mlp_dims, dropout).to(device)\n",
    "    elif model_name == 'DCN':\n",
    "        model = DCN(feature_nums, field_nums, embed_dim, num_layers, mlp_dims, dropout).to(device)\n",
    "    elif model_name == 'AFM':\n",
    "        model = AFM(feature_nums, field_nums, embed_dim, attn_size, dropout).to(device)\n",
    "    else:\n",
    "        raise ValueError('Unknown model name')\n",
    "\n",
    "    # Loss and optimizer\n",
    "    criterion = nn.BCEWithLogitsLoss()\n",
    "    optimizer = optim.Adam(model.parameters(), lr=1e-4, weight_decay=1e-5)\n",
    "\n",
    "    # Training with early stopping\n",
    "    train_model(model, train_loader, val_loader, criterion, optimizer, device, epochs, early_stopping_patience=5)\n",
    "\n",
    "    # Load best model\n",
    "    model.load_state_dict(torch.load('best_model.pth'))\n",
    "\n",
    "    # Configure model for CoTTA\n",
    "    model = configure_model(model)\n",
    "    params, param_names = collect_params(model)\n",
    "    optimizer = torch.optim.SGD(params, lr=0.0001)\n",
    "    cotta_model = CoTTA(model, optimizer, steps=1, episodic=False)\n",
    "\n",
    "    # Testing with CoTTA\n",
    "    model.eval()\n",
    "    y_true = []\n",
    "    y_scores = []\n",
    "\n",
    "    for x_batch, y_batch in test_loader:\n",
    "        x_batch = x_batch.to(device)\n",
    "        y_batch = y_batch.to(device)\n",
    "\n",
    "        outputs = cotta_model(x_batch)  # Outputs are logits\n",
    "        probabilities = torch.sigmoid(outputs).detach().cpu().numpy().flatten()\n",
    "\n",
    "        y_true.extend(y_batch.cpu().numpy())\n",
    "        y_scores.extend(probabilities)\n",
    "\n",
    "    test_auc = roc_auc_score(y_true, y_scores)\n",
    "    print(f'Test AUC with CoTTA: {test_auc:.4f}')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Training Function"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 421,
   "metadata": {},
   "outputs": [],
   "source": [
    "def train_model(model, train_loader, valid_loader, criterion, optimizer, device, epochs, early_stopping_patience=10):\n",
    "    best_loss = float('inf')\n",
    "    epochs_no_improve = 0\n",
    "    early_stop = False\n",
    "\n",
    "    for epoch in range(epochs):\n",
    "        if early_stop:\n",
    "            print(\"Early stopping\")\n",
    "            break\n",
    "\n",
    "        model.train()\n",
    "        total_loss = 0\n",
    "        for x_batch, y_batch in train_loader:\n",
    "            x_batch = x_batch.to(device)\n",
    "            y_batch = y_batch.to(device)\n",
    "            optimizer.zero_grad()\n",
    "            logits = model(x_batch).squeeze()\n",
    "            loss = criterion(logits, y_batch)\n",
    "            loss.backward()\n",
    "            optimizer.step()\n",
    "            total_loss += loss.item()\n",
    "        avg_loss = total_loss / len(train_loader)\n",
    "\n",
    "        # Validation\n",
    "        model.eval()\n",
    "        val_total_loss = 0\n",
    "        y_true = []\n",
    "        y_scores = []\n",
    "        with torch.no_grad():\n",
    "            for x_val, y_val in valid_loader:\n",
    "                x_val = x_val.to(device)\n",
    "                y_val = y_val.to(device)\n",
    "                logits = model(x_val).squeeze()\n",
    "                loss = criterion(logits, y_val)\n",
    "                val_total_loss += loss.item()\n",
    "                y_pred = torch.sigmoid(logits)\n",
    "                y_true.extend(y_val.cpu().numpy())\n",
    "                y_scores.extend(y_pred.cpu().numpy())\n",
    "        val_avg_loss = val_total_loss / len(valid_loader)\n",
    "        val_auc = roc_auc_score(y_true, y_scores)\n",
    "        print(f'Epoch {epoch+1}/{epochs}, Loss: {avg_loss:.4f}, Val Loss: {val_avg_loss:.4f}, Validation AUC: {val_auc:.4f}')\n",
    "\n",
    "        # Check for early stopping\n",
    "        if val_avg_loss < best_loss:\n",
    "            best_loss = val_avg_loss\n",
    "            epochs_no_improve = 0\n",
    "            # Save best model\n",
    "            torch.save(model.state_dict(), 'best_model.pth')\n",
    "        else:\n",
    "            epochs_no_improve += 1\n",
    "            if epochs_no_improve >= early_stopping_patience:\n",
    "                print(f\"Early stopping after {epoch+1} epochs\")\n",
    "                early_stop = True\n",
    "\n",
    "    print(f'Best Validation Loss: {best_loss:.4f}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 422,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/100, Loss: 0.3521, Val Loss: 0.0304, Validation AUC: 0.4908\n",
      "Epoch 2/100, Loss: 0.0204, Val Loss: 0.0107, Validation AUC: 0.4961\n",
      "Epoch 3/100, Loss: 0.0115, Val Loss: 0.0097, Validation AUC: 0.5011\n",
      "Epoch 4/100, Loss: 0.0104, Val Loss: 0.0095, Validation AUC: 0.5073\n",
      "Epoch 5/100, Loss: 0.0098, Val Loss: 0.0091, Validation AUC: 0.5141\n",
      "Epoch 6/100, Loss: 0.0093, Val Loss: 0.0087, Validation AUC: 0.5204\n",
      "Epoch 7/100, Loss: 0.0088, Val Loss: 0.0083, Validation AUC: 0.5260\n",
      "Epoch 8/100, Loss: 0.0083, Val Loss: 0.0080, Validation AUC: 0.5339\n",
      "Epoch 9/100, Loss: 0.0080, Val Loss: 0.0077, Validation AUC: 0.5414\n",
      "Epoch 10/100, Loss: 0.0077, Val Loss: 0.0075, Validation AUC: 0.5508\n",
      "Epoch 11/100, Loss: 0.0075, Val Loss: 0.0073, Validation AUC: 0.5616\n",
      "Epoch 12/100, Loss: 0.0073, Val Loss: 0.0072, Validation AUC: 0.5720\n",
      "Epoch 13/100, Loss: 0.0071, Val Loss: 0.0070, Validation AUC: 0.5827\n",
      "Epoch 14/100, Loss: 0.0069, Val Loss: 0.0069, Validation AUC: 0.5968\n",
      "Epoch 15/100, Loss: 0.0068, Val Loss: 0.0068, Validation AUC: 0.6089\n",
      "Epoch 16/100, Loss: 0.0067, Val Loss: 0.0067, Validation AUC: 0.6221\n",
      "Epoch 17/100, Loss: 0.0066, Val Loss: 0.0066, Validation AUC: 0.6346\n",
      "Epoch 18/100, Loss: 0.0065, Val Loss: 0.0065, Validation AUC: 0.6483\n",
      "Epoch 19/100, Loss: 0.0064, Val Loss: 0.0064, Validation AUC: 0.6596\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[422], line 2\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;18m__name__\u001b[39m \u001b[38;5;241m==\u001b[39m \u001b[38;5;124m'\u001b[39m\u001b[38;5;124m__main__\u001b[39m\u001b[38;5;124m'\u001b[39m:\n\u001b[0;32m----> 2\u001b[0m     \u001b[43mmain\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n",
      "Cell \u001b[0;32mIn[420], line 47\u001b[0m, in \u001b[0;36mmain\u001b[0;34m()\u001b[0m\n\u001b[1;32m     44\u001b[0m optimizer \u001b[38;5;241m=\u001b[39m optim\u001b[38;5;241m.\u001b[39mAdam(model\u001b[38;5;241m.\u001b[39mparameters(), lr\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m1e-4\u001b[39m, weight_decay\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m1e-5\u001b[39m)\n\u001b[1;32m     46\u001b[0m \u001b[38;5;66;03m# Training with early stopping\u001b[39;00m\n\u001b[0;32m---> 47\u001b[0m \u001b[43mtrain_model\u001b[49m\u001b[43m(\u001b[49m\u001b[43mmodel\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mtrain_loader\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mval_loader\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mcriterion\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43moptimizer\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mdevice\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mepochs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mearly_stopping_patience\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;241;43m5\u001b[39;49m\u001b[43m)\u001b[49m\n\u001b[1;32m     49\u001b[0m \u001b[38;5;66;03m# Load best model\u001b[39;00m\n\u001b[1;32m     50\u001b[0m model\u001b[38;5;241m.\u001b[39mload_state_dict(torch\u001b[38;5;241m.\u001b[39mload(\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mbest_model.pth\u001b[39m\u001b[38;5;124m'\u001b[39m))\n",
      "Cell \u001b[0;32mIn[421], line 17\u001b[0m, in \u001b[0;36mtrain_model\u001b[0;34m(model, train_loader, valid_loader, criterion, optimizer, device, epochs, early_stopping_patience)\u001b[0m\n\u001b[1;32m     15\u001b[0m y_batch \u001b[38;5;241m=\u001b[39m y_batch\u001b[38;5;241m.\u001b[39mto(device)\n\u001b[1;32m     16\u001b[0m optimizer\u001b[38;5;241m.\u001b[39mzero_grad()\n\u001b[0;32m---> 17\u001b[0m logits \u001b[38;5;241m=\u001b[39m \u001b[43mmodel\u001b[49m\u001b[43m(\u001b[49m\u001b[43mx_batch\u001b[49m\u001b[43m)\u001b[49m\u001b[38;5;241m.\u001b[39msqueeze()\n\u001b[1;32m     18\u001b[0m loss \u001b[38;5;241m=\u001b[39m criterion(logits, y_batch)\n\u001b[1;32m     19\u001b[0m loss\u001b[38;5;241m.\u001b[39mbackward()\n",
      "File \u001b[0;32m~/.local/lib/python3.10/site-packages/torch/nn/modules/module.py:1532\u001b[0m, in \u001b[0;36mModule._wrapped_call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1530\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_compiled_call_impl(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)  \u001b[38;5;66;03m# type: ignore[misc]\u001b[39;00m\n\u001b[1;32m   1531\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m-> 1532\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_call_impl\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/.local/lib/python3.10/site-packages/torch/nn/modules/module.py:1541\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1536\u001b[0m \u001b[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[1;32m   1537\u001b[0m \u001b[38;5;66;03m# this function, and just call forward.\u001b[39;00m\n\u001b[1;32m   1538\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_pre_hooks\n\u001b[1;32m   1539\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_backward_hooks\n\u001b[1;32m   1540\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[0;32m-> 1541\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mforward_call\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1543\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[1;32m   1544\u001b[0m     result \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m\n",
      "Cell \u001b[0;32mIn[419], line 32\u001b[0m, in \u001b[0;36mAFM.forward\u001b[0;34m(self, x)\u001b[0m\n\u001b[1;32m     30\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m i \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mrange\u001b[39m(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mfield_nums):\n\u001b[1;32m     31\u001b[0m     \u001b[38;5;28;01mfor\u001b[39;00m j \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mrange\u001b[39m(i \u001b[38;5;241m+\u001b[39m \u001b[38;5;241m1\u001b[39m, \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mfield_nums):\n\u001b[0;32m---> 32\u001b[0m         \u001b[43minteractions\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mappend\u001b[49m\u001b[43m(\u001b[49m\u001b[43mx_embed\u001b[49m\u001b[43m[\u001b[49m\u001b[43m:\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mi\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m:\u001b[49m\u001b[43m]\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43m \u001b[49m\u001b[43mx_embed\u001b[49m\u001b[43m[\u001b[49m\u001b[43m:\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mj\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m:\u001b[49m\u001b[43m]\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m     33\u001b[0m interactions \u001b[38;5;241m=\u001b[39m torch\u001b[38;5;241m.\u001b[39mstack(interactions, dim\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m1\u001b[39m)  \u001b[38;5;66;03m# Shape: (batch_size, num_pairs, embed_dim)\u001b[39;00m\n\u001b[1;32m     35\u001b[0m \u001b[38;5;66;03m# Attention mechanism\u001b[39;00m\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "if __name__ == '__main__':\n",
    "    main()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
